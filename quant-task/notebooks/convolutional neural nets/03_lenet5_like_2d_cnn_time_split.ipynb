{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a85957",
   "metadata": {},
   "source": [
    "# LeNet5-like CNN on Feature Grid\n",
    "\n",
    "These notebooks implement the Chapter 18 CNN models (time series as images) on this repo's dataset.\n",
    "\n",
    "Common requirements satisfied in each notebook:\n",
    "- **Time-wise split**: train = first 7 years, val = middle, test = last 18 months\n",
    "- Uses the existing backtester: `src/backtester.engine.run_backtest`\n",
    "- Uses the existing Bokeh dashboard: `src/backtester.bokeh_plots.build_interactive_portfolio_layout`\n",
    "- Backtest window is sliced to the test period so reported Start/End align with the test start (not 2016).\n",
    "\n",
    "GPU note (AMD): PyTorch uses `torch.cuda` for both CUDA and ROCm builds. If your PyTorch build supports ROCm, `torch.cuda.is_available()` should be True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90167153",
   "metadata": {},
   "source": [
    "## Math: Convolution and Pooling\n",
    "\n",
    "For a 1D signal x and kernel w, a discrete 1D convolution (cross-correlation in many DL libs) is:\n",
    "\n",
    "$$y[t] = \\sum_{k=0}^{K-1} w[k] \\cdot x[t+k].$$\n",
    "\n",
    "For multichannel inputs, the kernel has shape (C_in, K), and outputs sum over channels.\n",
    "\n",
    "Pooling reduces resolution while keeping salient local patterns. For example, max-pooling over window size P:\n",
    "\n",
    "$$y[t] = \\max_{0\\le k < P} x[t+k].$$\n",
    "\n",
    "CNNs exploit locality and weight sharing to reduce parameters vs fully-connected layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59352ec6",
   "metadata": {},
   "source": [
    "## Data, Label, and Portfolio Construction\n",
    "\n",
    "We load the repo's feature-extracted panel dataset and create a forward label per asset:\n",
    "\n",
    "$$y_t = r_{t+1}$$\n",
    "\n",
    "where `ret_1d` is the known one-day return at time t.\n",
    "\n",
    "Binary direction label:\n",
    "\n",
    "$$y^{\\uparrow}_t = \\mathbb{1}[y_t > 0].$$\n",
    "\n",
    "From a model probability $p_{i,t}$, define a centered score $s_{i,t} = p_{i,t} - 0.5$.\n",
    "\n",
    "Weekly Top-K long-only weights at rebalance date t:\n",
    "\n",
    "$$w_{i,t}=\\frac{1}{|\\mathcal{L}_t|}\\mathbb{1}[i\\in\\mathcal{L}_t],\\quad \\mathcal{L}_t=\\{\\text{TopK}(s_{\\cdot,t})\\cap (s_{\\cdot,t}>0)\\}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb8ea7",
   "metadata": {},
   "source": [
    "## LeNet5 (Adapted) - Math/Architecture\n",
    "\n",
    "LeNet5 uses stacked Conv2D + pooling + fully connected layers.\n",
    "We adapt it to 15x15 single-channel inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccbe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from bokeh.io import output_notebook, show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve project root robustly by walking parents.\n",
    "CWD = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "for p in [CWD, *CWD.parents]:\n",
    "    if (p / 'dataset').exists() and (p / 'src').exists():\n",
    "        PROJECT_ROOT = p\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    raise RuntimeError(f'Could not locate project root from CWD={CWD}')\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection: CUDA on NVIDIA, ROCm on AMD (if your PyTorch build supports it).\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('torch version:', torch.__version__)\n",
    "print('device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53037324",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "FEATURES_PARQUET_PATH = PROJECT_ROOT / 'dataset' / 'features' / 'all_features.parquet'\n",
    "FEATURES_CSV_PATH = PROJECT_ROOT / 'dataset' / 'features' / 'all_features.csv'\n",
    "\n",
    "TARGET_COL = 'ret_1d'\n",
    "TARGET_FWD_COL = 'y_ret_1d_fwd'\n",
    "TARGET_DIR_COL = 'y_up_fwd'\n",
    "\n",
    "TRAIN_YEARS = 7\n",
    "TEST_MONTHS = 18\n",
    "\n",
    "REBALANCE_FREQ = 'W'\n",
    "TOP_K = 20\n",
    "INITIAL_EQUITY = 1_000_000.0\n",
    "TXN_COST_BPS = 5.0\n",
    "\n",
    "\n",
    "def load_feature_dataset() -> pd.DataFrame:\n",
    "    if FEATURES_PARQUET_PATH.exists():\n",
    "        df = pd.read_parquet(FEATURES_PARQUET_PATH)\n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df = df.set_index('Date')\n",
    "    elif FEATURES_CSV_PATH.exists():\n",
    "        df = pd.read_csv(FEATURES_CSV_PATH, parse_dates=['Date']).set_index('Date')\n",
    "    else:\n",
    "        raise FileNotFoundError('Feature dataset not found under dataset/features/.')\n",
    "\n",
    "    required = {'Asset_ID', TARGET_COL}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f'Missing required columns: {sorted(missing)}')\n",
    "\n",
    "    return df.sort_index()\n",
    "\n",
    "panel = load_feature_dataset().copy()\n",
    "\n",
    "panel[TARGET_FWD_COL] = panel.groupby('Asset_ID', sort=False)[TARGET_COL].shift(-1)\n",
    "panel = panel.dropna(subset=[TARGET_FWD_COL]).sort_index()\n",
    "panel[TARGET_DIR_COL] = (panel[TARGET_FWD_COL].astype(float) > 0.0).astype(int)\n",
    "\n",
    "start = pd.Timestamp(panel.index.min())\n",
    "end = pd.Timestamp(panel.index.max())\n",
    "train_end = start + pd.DateOffset(years=TRAIN_YEARS)\n",
    "test_start = end - pd.DateOffset(months=TEST_MONTHS)\n",
    "\n",
    "if train_end >= test_start:\n",
    "    raise ValueError(\n",
    "        f'Not enough history for requested split: start={start.date()} train_end={train_end.date()} test_start={test_start.date()} end={end.date()}'\n",
    "    )\n",
    "\n",
    "train_mask = panel.index < train_end\n",
    "val_mask = (panel.index >= train_end) & (panel.index < test_start)\n",
    "test_mask = panel.index >= test_start\n",
    "\n",
    "panel_train = panel.loc[train_mask].copy()\n",
    "panel_val = panel.loc[val_mask].copy()\n",
    "panel_test = panel.loc[test_mask].copy()\n",
    "\n",
    "print('date range:', start.date(), '->', end.date())\n",
    "print('train:', panel_train.index.min().date(), '->', panel_train.index.max().date(), 'rows:', panel_train.shape[0])\n",
    "print('val  :', panel_val.index.min().date(), '->', panel_val.index.max().date(), 'rows:', panel_val.shape[0])\n",
    "print('test :', panel_test.index.min().date(), '->', panel_test.index.max().date(), 'rows:', panel_test.shape[0])\n",
    "print('assets:', panel['Asset_ID'].nunique())\n",
    "\n",
    "exclude = {'Asset_ID', TARGET_FWD_COL, TARGET_DIR_COL}\n",
    "feature_cols = [c for c in panel.columns if c not in exclude]\n",
    "numeric_feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(panel[c])]\n",
    "print('n_numeric_features:', len(numeric_feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Grid size adapts to available numeric features.\n",
    "# Our feature dataset may have fewer than 15x15=225 numeric features.\n",
    "N_AVAIL = len(numeric_feature_cols)\n",
    "GRID = int(np.floor(np.sqrt(N_AVAIL)))\n",
    "N_PIXELS = GRID * GRID\n",
    "if N_PIXELS < 25:\n",
    "    raise ValueError(f'Not enough numeric features for CNN grid: n={N_AVAIL}')\n",
    "print('grid:', GRID, 'x', GRID, 'using', N_PIXELS, 'of', N_AVAIL, 'features')\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 3\n",
    "LR = 1e-3\n",
    "\n",
    "# Select top features on training window\n",
    "X_fs = panel_train[numeric_feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "y_fs = panel_train[TARGET_DIR_COL].astype(int)\n",
    "med = X_fs.median(axis=0)\n",
    "X_imp = X_fs.fillna(med)\n",
    "\n",
    "mi = mutual_info_classif(X_imp.to_numpy(), y_fs.to_numpy(), random_state=SEED)\n",
    "mi_s = pd.Series(mi, index=numeric_feature_cols).sort_values(ascending=False)\n",
    "sel = mi_s.head(N_PIXELS).index.tolist()\n",
    "print('selected', len(sel), 'features for', GRID, 'x', GRID)\n",
    "\n",
    "# Cluster/order features so correlated features are near each other in the grid\n",
    "corr = X_imp.loc[:, sel].corr().fillna(0.0)\n",
    "dist = (1.0 - corr.abs()).clip(lower=0.0)\n",
    "Z = linkage(squareform(dist.values, checks=False), method='average')\n",
    "order = leaves_list(Z)\n",
    "sel_ordered = [sel[i] for i in order]\n",
    "\n",
    "# Train normalization stats\n",
    "train_vals = panel_train.loc[:, sel_ordered].replace([np.inf, -np.inf], np.nan)\n",
    "train_med = train_vals.median(axis=0)\n",
    "train_mean = train_vals.fillna(train_med).mean(axis=0)\n",
    "train_std = train_vals.fillna(train_med).std(axis=0, ddof=0).replace(0.0, 1.0)\n",
    "\n",
    "\n",
    "def make_images(df: pd.DataFrame):\n",
    "    X = df.loc[:, sel_ordered].replace([np.inf, -np.inf], np.nan).fillna(train_med)\n",
    "    X = ((X - train_mean) / train_std).to_numpy(dtype=np.float32)\n",
    "    X = X.reshape((len(df), 1, GRID, GRID))\n",
    "    y = df[TARGET_DIR_COL].astype(int).to_numpy(dtype=np.int64)\n",
    "    dates = df.index.to_numpy(dtype='datetime64[ns]')\n",
    "    assets = df['Asset_ID'].to_numpy()\n",
    "    return X, y, dates, assets\n",
    "\n",
    "X_tr, y_tr, d_tr, a_tr = make_images(panel_train.loc[:, ['Asset_ID', TARGET_DIR_COL] + sel_ordered])\n",
    "X_va, y_va, d_va, a_va = make_images(panel_val.loc[:, ['Asset_ID', TARGET_DIR_COL] + sel_ordered])\n",
    "X_te, y_te, d_te, a_te = make_images(panel_test.loc[:, ['Asset_ID', TARGET_DIR_COL] + sel_ordered])\n",
    "\n",
    "print('X_tr:', X_tr.shape, 'X_te:', X_te.shape)\n",
    "\n",
    "class ImgDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(ImgDS(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(ImgDS(X_va, y_va), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class LeNetLike(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=3, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(6, 16, kernel_size=3, padding=0),\n",
    "            nn.Tanh(),\n",
    "            # Make output shape independent of GRID\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "model = LeNetLike().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.float().to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_p, all_y = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            p = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            all_p.append(p)\n",
    "            all_y.append(yb.numpy())\n",
    "    if all_p:\n",
    "        import sklearn.metrics as skm\n",
    "        p = np.concatenate(all_p)\n",
    "        y = np.concatenate(all_y)\n",
    "        if len(np.unique(y)) > 1:\n",
    "            print('epoch', epoch, 'val_auc', float(skm.roc_auc_score(y, p)))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.from_numpy(X_te).to(device)).cpu().numpy()\n",
    "proba = 1.0 / (1.0 + np.exp(-logits))\n",
    "\n",
    "MODEL_TITLE = 'LeNet5-like 2D CNN on Feature Grid - Time Split'\n",
    "\n",
    "dates_test = pd.to_datetime(d_te)\n",
    "assets_test = a_te\n",
    "scores_test = proba - 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a608563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.backtester.data import load_cleaned_assets, align_close_prices\n",
    "from src.backtester.engine import BacktestConfig, run_backtest\n",
    "from src.backtester.report import compute_backtest_report\n",
    "from src.backtester.bokeh_plots import build_interactive_portfolio_layout\n",
    "from src.backtester.portfolio import equal_weight\n",
    "\n",
    "# Convert per-row predictions to pred_matrix[date, asset]\n",
    "long = pd.DataFrame({'Date': dates_test, 'Asset_ID': assets_test, 'score': scores_test})\n",
    "pred_matrix = long.pivot_table(index='Date', columns='Asset_ID', values='score', aggfunc='mean').sort_index()\n",
    "\n",
    "bt_assets = sorted(panel['Asset_ID'].unique().tolist())\n",
    "assets_ohlcv = load_cleaned_assets(symbols=bt_assets, cleaned_dir=str(PROJECT_ROOT / 'dataset' / 'cleaned'))\n",
    "close_prices = align_close_prices(assets_ohlcv).sort_index()\n",
    "\n",
    "# Slice backtest to TEST window (fixes Start not being 2016)\n",
    "bt_start = pd.Timestamp(panel_test.index.min())\n",
    "bt_end = pd.Timestamp(panel_test.index.max())\n",
    "close_prices = close_prices.loc[bt_start:bt_end]\n",
    "\n",
    "assets_ohlcv_slice = {k: v.loc[bt_start:bt_end] for k, v in assets_ohlcv.items()}\n",
    "market_df = pd.DataFrame({\n",
    "    'Open': pd.concat([d['Open'] for d in assets_ohlcv_slice.values()], axis=1).mean(axis=1),\n",
    "    'High': pd.concat([d['High'] for d in assets_ohlcv_slice.values()], axis=1).mean(axis=1),\n",
    "    'Low': pd.concat([d['Low'] for d in assets_ohlcv_slice.values()], axis=1).mean(axis=1),\n",
    "    'Close': pd.concat([d['Close'] for d in assets_ohlcv_slice.values()], axis=1).mean(axis=1),\n",
    "    'Volume': pd.concat([d['Volume'] for d in assets_ohlcv_slice.values()], axis=1).sum(axis=1),\n",
    "}).sort_index()\n",
    "\n",
    "pred_matrix = pred_matrix.reindex(close_prices.index)\n",
    "output_notebook()\n",
    "\n",
    "rebal_dates = set(pd.Series(pred_matrix.index, index=pred_matrix.index).resample(REBALANCE_FREQ).last().dropna().tolist())\n",
    "\n",
    "w_last = pd.Series(0.0, index=bt_assets)\n",
    "w_rows = []\n",
    "for dt in pred_matrix.index:\n",
    "    if dt in rebal_dates:\n",
    "        row = pred_matrix.loc[dt].dropna().sort_values(ascending=False)\n",
    "        top = row.head(min(TOP_K, len(row)))\n",
    "        candidates = [a for a, v in top.items() if np.isfinite(v) and float(v) > 0.0]\n",
    "        if len(candidates) == 0:\n",
    "            w_last = pd.Series(0.0, index=bt_assets)\n",
    "        else:\n",
    "            w_dict = equal_weight(candidates)\n",
    "            w_last = pd.Series(0.0, index=bt_assets)\n",
    "            for a, w in w_dict.items():\n",
    "                if a in w_last.index:\n",
    "                    w_last[a] = float(w)\n",
    "    w_rows.append(w_last)\n",
    "\n",
    "weights = pd.DataFrame(w_rows, index=pred_matrix.index, columns=bt_assets).fillna(0.0)\n",
    "\n",
    "cfg = BacktestConfig(initial_equity=INITIAL_EQUITY, transaction_cost_bps=TXN_COST_BPS, mode='vectorized')\n",
    "res = run_backtest(close_prices, weights, config=cfg)\n",
    "report = compute_backtest_report(result=res, close_prices=close_prices)\n",
    "display(report.to_frame(MODEL_TITLE))\n",
    "\n",
    "layout = build_interactive_portfolio_layout(\n",
    "    market_ohlcv=market_df,\n",
    "    equity=res.equity,\n",
    "    returns=res.returns,\n",
    "    weights=res.weights,\n",
    "    turnover=res.turnover,\n",
    "    costs=res.costs,\n",
    "    close_prices=close_prices,\n",
    "    title=MODEL_TITLE,\n",
    ")\n",
    "show(layout)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
