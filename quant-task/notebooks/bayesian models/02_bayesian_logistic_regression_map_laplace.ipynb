{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed381f2",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression (MAP + Laplace Approximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1091133",
   "metadata": {},
   "source": [
    "We model $y \\in \\{0,1\\}$ (next-day up move) using logistic regression with a Gaussian prior.\n",
    "\n",
    "- Likelihood: $p(y=1|x,\\\\beta)=\\\\sigma(x^T\\\\beta)$\n",
    "- Prior: $\\\\beta \\sim \\mathcal{N}(0, \\tau^2 I)$\n",
    "- MAP: maximize $\\\\log p(\\\\beta|D) = \\sum_i \\log p(y_i|x_i,\\\\beta) - \\frac{1}{2\\\\tau^2}\\\\|\\\\beta\\\\|^2$\n",
    "\n",
    "Laplace approximation uses the Hessian at the MAP estimate to approximate\n",
    "$p(\\beta|D)$ as Gaussian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80e18a",
   "metadata": {},
   "source": [
    "# Chapter 10 Bayesian ML (Predictive): Bayesian Logistic Regression (MAP)\n",
    "\n",
    "These notebooks mirror the *methods* highlighted in\n",
    "`ml_finance_thoery/machine-learning-for-trading/10_bayesian_machine_learning/README.md`\n",
    "and apply them to the local `dataset/cleaned/` asset universe to produce\n",
    "out-of-sample predictions and a backtest using the same **vectorized** engine\n",
    "used by the `notebooks/ML_Linear_Models_*` notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    p = start.resolve()\n",
    "    for _ in range(10):\n",
    "        if (p / 'src').exists() and (p / 'dataset').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError(f'Could not find project root from: {start!s}')\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "CLEANED_DIR = PROJECT_ROOT / 'dataset' / 'cleaned'\n",
    "\n",
    "# Ensure `src/` is on sys.path so `backtester` is importable\n",
    "src_dir = PROJECT_ROOT / 'src'\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.append(str(src_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40067692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature set: reuse the same SMC + OHLCV engineering pattern from ML_linear_models_05\n",
    "# but keep it in-notebook for portability.\n",
    "\n",
    "SWING_WINDOW = 5\n",
    "N_CONSEC_SWINGS = 3\n",
    "\n",
    "def compute_features(ohlcv: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = ohlcv.copy().sort_index()\n",
    "    o = df['Open'].astype(float)\n",
    "    h = df['High'].astype(float)\n",
    "    l = df['Low'].astype(float)\n",
    "    c = df['Close'].astype(float)\n",
    "    v = df['Volume'].astype(float)\n",
    "    ret_1d = c.pct_change()\n",
    "    hl_range = (h - l)\n",
    "    hl_range_pct = hl_range / c.replace(0.0, np.nan)\n",
    "    body = (c - o)\n",
    "    body_pct = body / c.replace(0.0, np.nan)\n",
    "    hlc3 = (h + l + c) / 3.0\n",
    "    vol_z_20 = (v - v.rolling(20).mean()) / v.rolling(20).std()\n",
    "\n",
    "    w = int(SWING_WINDOW)\n",
    "    win = 2 * w + 1\n",
    "    swing_high = h.eq(h.rolling(win, center=True, min_periods=win).max())\n",
    "    swing_low = l.eq(l.rolling(win, center=True, min_periods=win).min())\n",
    "    last_swing_high = h.where(swing_high).ffill()\n",
    "    last_swing_low = l.where(swing_low).ffill()\n",
    "    prev_high = last_swing_high.shift(1)\n",
    "    prev_low = last_swing_low.shift(1)\n",
    "    bos_bull = (c > prev_high) & (c.shift(1) <= prev_high) & prev_high.notna()\n",
    "    bos_bear = (c < prev_low) & (c.shift(1) >= prev_low) & prev_low.notna()\n",
    "\n",
    "    # Simplified CHOCH proxy (trend not explicitly inferred here)\n",
    "    choch_bull = bos_bull.astype(int)\n",
    "    choch_bear = bos_bear.astype(int)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        'open': o,\n",
    "        'high': h,\n",
    "        'low': l,\n",
    "        'close': c,\n",
    "        'volume': v,\n",
    "        'ret_1d': ret_1d,\n",
    "        'hl_range': hl_range,\n",
    "        'hl_range_pct': hl_range_pct,\n",
    "        'body': body,\n",
    "        'body_pct': body_pct,\n",
    "        'hlc3': hlc3,\n",
    "        'vol_z_20': vol_z_20,\n",
    "        'bos_bull': bos_bull.astype(int),\n",
    "        'bos_bear': bos_bear.astype(int),\n",
    "        'choch_bull': choch_bull,\n",
    "        'choch_bear': choch_bear,\n",
    "    }).sort_index()\n",
    "    out['ret_1d_fwd'] = out['ret_1d'].shift(-1)\n",
    "    out['y_up_fwd'] = (out['ret_1d_fwd'] > 0).astype(int)\n",
    "    return out\n",
    "\n",
    "# Load data and build a long panel (Date index, Asset_ID column)\n",
    "from backtester.data import load_cleaned_assets\n",
    "\n",
    "files = sorted([p for p in CLEANED_DIR.glob('Asset_*.csv')])\n",
    "symbols = [p.stem for p in files]\n",
    "assets_ohlcv = load_cleaned_assets(symbols=symbols, cleaned_dir=str(CLEANED_DIR))\n",
    "\n",
    "frames = []\n",
    "for sym, df in assets_ohlcv.items():\n",
    "    feat = compute_features(df)\n",
    "    feat['Asset_ID'] = sym\n",
    "    frames.append(feat)\n",
    "\n",
    "data = pd.concat(frames, axis=0).sort_index()\n",
    "data = data.dropna(subset=['y_up_fwd'])\n",
    "\n",
    "# Time split (same policy as ML_linear_models notebooks)\n",
    "TRAIN_YEARS = 7\n",
    "VAL_MONTHS = 18\n",
    "TEST_MONTHS = 18\n",
    "\n",
    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    pos = int(index.searchsorted(ts, side='left'))\n",
    "    if pos >= len(index):\n",
    "        return pd.Timestamp(index[-1])\n",
    "    return pd.Timestamp(index[pos])\n",
    "\n",
    "idx = pd.DatetimeIndex(data.index.unique()).sort_values()\n",
    "end = pd.Timestamp(idx[-1])\n",
    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n",
    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n",
    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n",
    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n",
    "val_start = align_to_trading_date(idx, pd.Timestamp(raw_val_start))\n",
    "train_start = align_to_trading_date(idx, pd.Timestamp(raw_train_start))\n",
    "print('aligned boundaries:', train_start, val_start, test_start, end)\n",
    "\n",
    "df_train = data.loc[(data.index >= train_start) & (data.index < val_start)].copy()\n",
    "df_val = data.loc[(data.index >= val_start) & (data.index < test_start)].copy()\n",
    "df_test = data.loc[(data.index >= test_start) & (data.index <= end)].copy()\n",
    "\n",
    "feature_cols = [c for c in df_train.columns if c not in {'Asset_ID', 'ret_1d_fwd', 'y_up_fwd'} and pd.api.types.is_numeric_dtype(df_train[c])]\n",
    "X_train = df_train[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "y_train = df_train['y_up_fwd'].astype(int).to_numpy()\n",
    "X_test = df_test[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Preprocess (impute + standardize)\n",
    "imp = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_imp = imp.fit_transform(X_train)\n",
    "X_train_std = scaler.fit_transform(X_train_imp)\n",
    "X_test_std = scaler.transform(imp.transform(X_test))\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# Add intercept\n",
    "Xtr = np.hstack([np.ones((X_train_std.shape[0], 1)), X_train_std])\n",
    "Xte = np.hstack([np.ones((X_test_std.shape[0], 1)), X_test_std])\n",
    "\n",
    "TAU2 = 10.0**2  # prior variance\n",
    "\n",
    "def neg_log_posterior(beta: np.ndarray) -> float:\n",
    "    z = Xtr @ beta\n",
    "    p = sigmoid(z)\n",
    "    eps = 1e-12\n",
    "    ll = np.sum(y_train * np.log(p + eps) + (1 - y_train) * np.log(1 - p + eps))\n",
    "    lp = -0.5 * np.sum(beta[1:] ** 2) / TAU2  # no prior on intercept\n",
    "    return float(-(ll + lp))\n",
    "\n",
    "def grad_neg_log_posterior(beta: np.ndarray) -> np.ndarray:\n",
    "    z = Xtr @ beta\n",
    "    p = sigmoid(z)\n",
    "    g = Xtr.T @ (p - y_train)\n",
    "    g[1:] += beta[1:] / TAU2\n",
    "    return g\n",
    "\n",
    "beta0 = np.zeros(Xtr.shape[1])\n",
    "res = minimize(neg_log_posterior, beta0, jac=grad_neg_log_posterior, method='BFGS')\n",
    "if not res.success:\n",
    "    raise RuntimeError(res.message)\n",
    "beta_map = res.x\n",
    "cov_laplace = res.hess_inv\n",
    "print('MAP beta shape:', beta_map.shape)\n",
    "\n",
    "# Posterior predictive via Monte Carlo from Laplace approximation\n",
    "S = 200\n",
    "beta_samps = rng.multivariate_normal(mean=beta_map, cov=cov_laplace, size=S)\n",
    "p_samps = sigmoid(Xte @ beta_samps.T)\n",
    "p_mean = p_samps.mean(axis=1)\n",
    "\n",
    "# Convert to per-date, per-asset prediction matrix\n",
    "pred_long = pd.DataFrame({\n",
    "    'Date': df_test.index,\n",
    "    'Asset_ID': df_test['Asset_ID'].to_numpy(),\n",
    "    'p_up': p_mean,\n",
    "})\n",
    "pred_long['signal'] = pred_long['p_up'] - 0.5\n",
    "pred_matrix = pred_long.pivot_table(index='Date', columns='Asset_ID', values='signal', aggfunc='mean').sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "from backtester.data import load_cleaned_assets, align_close_prices\n",
    "from backtester.engine import BacktestConfig, run_backtest\n",
    "from backtester.report import compute_backtest_report\n",
    "from backtester.bokeh_plots import build_interactive_portfolio_layout\n",
    "from backtester.portfolio import equal_weight, optimize_mpt\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "if 'pred_matrix' not in globals():\n",
    "    raise RuntimeError('Expected `pred_matrix` (index=date, columns=Asset_ID) to exist')\n",
    "\n",
    "# IMPORTANT: performance metrics should start when the model has signals.\n",
    "# Keep the original prediction date range before reindexing to market data.\n",
    "pred_range = pd.DatetimeIndex(pred_matrix.index).sort_values()\n",
    "if pred_range.empty:\n",
    "    raise RuntimeError('pred_matrix has empty index')\n",
    "bt_start = pd.Timestamp(pred_range[0])\n",
    "bt_end = pd.Timestamp(pred_range[-1])\n",
    "\n",
    "bt_assets = sorted([str(c) for c in pred_matrix.columns.tolist()])\n",
    "assets_ohlcv = load_cleaned_assets(symbols=bt_assets, cleaned_dir=str(CLEANED_DIR))\n",
    "close_prices = align_close_prices(assets_ohlcv)\n",
    "\n",
    "# Align prediction matrix to available trade dates\n",
    "pred_matrix = pred_matrix.reindex(close_prices.index)\n",
    "# Restrict backtest to the prediction window (avoid 2016-start metrics).\n",
    "close_prices = close_prices.loc[bt_start:bt_end]\n",
    "pred_matrix = pred_matrix.loc[bt_start:bt_end]\n",
    "returns_matrix = close_prices.pct_change().fillna(0.0)\n",
    "\n",
    "market_df = pd.DataFrame({\n",
    "    'Open': pd.concat([df['Open'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n",
    "    'High': pd.concat([df['High'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n",
    "    'Low': pd.concat([df['Low'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n",
    "    'Close': pd.concat([df['Close'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n",
    "    'Volume': pd.concat([df['Volume'] for df in assets_ohlcv.values()], axis=1).sum(axis=1),\n",
    "}).sort_index().loc[bt_start:bt_end]\n",
    "\n",
    "REBALANCE_FREQ = 'W'\n",
    "TOP_K = min(20, len(bt_assets))\n",
    "LOOKBACK_DAYS = 126\n",
    "\n",
    "def build_weights_from_predictions(pred_matrix: pd.DataFrame, *, pm_style: str) -> pd.DataFrame:\n",
    "    rebal_dates = set(pd.Series(pred_matrix.index, index=pred_matrix.index).resample(REBALANCE_FREQ).last().dropna().tolist())\n",
    "    w_last = pd.Series(0.0, index=bt_assets)\n",
    "    rows = []\n",
    "    for dt in pred_matrix.index:\n",
    "        if dt in rebal_dates:\n",
    "            row = pred_matrix.loc[dt].dropna().sort_values(ascending=False)\n",
    "            top = row.head(TOP_K)\n",
    "            candidates = [a for a, v in top.items() if np.isfinite(v) and float(v) > 0.0]\n",
    "            if not candidates:\n",
    "                w_last = pd.Series(0.0, index=bt_assets)\n",
    "            else:\n",
    "                if pm_style == '1N':\n",
    "                    w_dict = equal_weight(candidates)\n",
    "                elif pm_style == 'MPT':\n",
    "                    w_dict = optimize_mpt(returns_matrix, candidates, dt, lookback_days=LOOKBACK_DAYS)\n",
    "                else:\n",
    "                    raise ValueError(f'Unknown pm_style: {pm_style!r}')\n",
    "                w_last = pd.Series(0.0, index=bt_assets)\n",
    "                for a, w in w_dict.items():\n",
    "                    w_last[str(a)] = float(w)\n",
    "        rows.append(w_last)\n",
    "    return pd.DataFrame(rows, index=pred_matrix.index, columns=bt_assets).fillna(0.0)\n",
    "\n",
    "cfg = BacktestConfig(initial_equity=1_000_000.0, transaction_cost_bps=5.0, mode='vectorized')\n",
    "\n",
    "compare_rows = []\n",
    "results = {}\n",
    "for pm_style in ['1N', 'MPT']:\n",
    "    w = build_weights_from_predictions(pred_matrix, pm_style=pm_style)\n",
    "    res = run_backtest(close_prices, w, config=cfg)\n",
    "    rpt = compute_backtest_report(result=res, close_prices=close_prices)\n",
    "    results[pm_style] = (w, res, rpt)\n",
    "    compare_rows.append({\n",
    "        'style': pm_style,\n",
    "        'Total Return [%]': float(rpt['Total Return [%]']),\n",
    "        'CAGR [%]': float(rpt['CAGR [%]']),\n",
    "        'Sharpe': float(rpt['Sharpe']),\n",
    "        'Max Drawdown [%]': float(rpt['Max Drawdown [%]']),\n",
    "    })\n",
    "compare = pd.DataFrame(compare_rows).sort_values('Total Return [%]', ascending=False).reset_index(drop=True)\n",
    "display(compare)\n",
    "\n",
    "BASE_TITLE = 'Bayes Logistic Regression (MAP+Laplace)'\n",
    "for pm_style in ['1N', 'MPT']:\n",
    "    w, res, rpt = results[pm_style]\n",
    "    title = BASE_TITLE + ' - ' + pm_style\n",
    "    display(rpt.to_frame(title))\n",
    "    layout = build_interactive_portfolio_layout(\n",
    "        market_ohlcv=market_df,\n",
    "        equity=res.equity,\n",
    "        returns=res.returns,\n",
    "        weights=res.weights,\n",
    "        turnover=res.turnover,\n",
    "        costs=res.costs,\n",
    "        close_prices=close_prices,\n",
    "        title=title,\n",
    "    )\n",
    "    show(layout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
