{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e90cb4",
   "metadata": {},
   "source": [
    "# FinMamba (Market-Aware Graph + Multi-Level Mamba) - From PDF\n",
    "\n",
    "This notebook implements the core components described in `finmamba.pdf`:\n",
    "- Market-Aware Graph (MAG): dynamic stock correlation graph + market-aware sparsification using a market proxy index.\n",
    "- Graph Attention Aggregation: multi-head neighbor aggregation on the pruned graph.\n",
    "- Multi-Level Mamba (MLM): multi-level selective SSM blocks.\n",
    "- Optimization objectives: point-wise regression + pair-wise hinge ranking + GIB loss.\n",
    "\n",
    "Data:\n",
    "- Uses the repo feature dataset: `dataset/features/all_features.parquet` (fallback: `all_features.csv`).\n",
    "\n",
    "Training:\n",
    "- Time split: train first 7 years, validate middle, test last 18 months.\n",
    "- Epochs: 3.\n",
    "\n",
    "Backtest:\n",
    "- Converts predicted scores into weekly Top-K long-only weights.\n",
    "- Runs `src/backtester.engine.run_backtest` and shows the existing Bokeh dashboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve project root robustly\n",
    "CWD = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "for p in [CWD, *CWD.parents]:\n",
    "    if (p / 'dataset').exists() and (p / 'src').exists():\n",
    "        PROJECT_ROOT = p\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    raise RuntimeError(f'Could not locate project root from CWD={CWD}')\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5177c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('torch:', torch.__version__)\n",
    "print('device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7873c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature-extracted dataset\n",
    "FEATURES_PARQUET_PATH = PROJECT_ROOT / 'dataset' / 'features' / 'all_features.parquet'\n",
    "FEATURES_CSV_PATH = PROJECT_ROOT / 'dataset' / 'features' / 'all_features.csv'\n",
    "\n",
    "if FEATURES_PARQUET_PATH.exists():\n",
    "    df = pd.read_parquet(FEATURES_PARQUET_PATH)\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.set_index('Date')\n",
    "elif FEATURES_CSV_PATH.exists():\n",
    "    df = pd.read_csv(FEATURES_CSV_PATH, parse_dates=['Date']).set_index('Date')\n",
    "else:\n",
    "    raise FileNotFoundError('Feature dataset not found under dataset/features/.')\n",
    "\n",
    "if 'Asset_ID' not in df.columns:\n",
    "    raise ValueError('Expected Asset_ID column in feature dataset')\n",
    "if 'ret_1d' not in df.columns:\n",
    "    raise ValueError('Expected ret_1d column in feature dataset')\n",
    "\n",
    "df = df.sort_index()\n",
    "\n",
    "# Forward labels\n",
    "TARGET_FWD_COL = 'y_ret_1d_fwd'\n",
    "df[TARGET_FWD_COL] = df.groupby('Asset_ID', sort=False)['ret_1d'].shift(-1)\n",
    "df = df.dropna(subset=[TARGET_FWD_COL])\n",
    "\n",
    "# Use forward return for regression target r_t\n",
    "\n",
    "TRAIN_YEARS = 7\n",
    "TEST_MONTHS = 18\n",
    "\n",
    "start = pd.Timestamp(df.index.min())\n",
    "end = pd.Timestamp(df.index.max())\n",
    "train_end = start + pd.DateOffset(years=TRAIN_YEARS)\n",
    "test_start = end - pd.DateOffset(months=TEST_MONTHS)\n",
    "\n",
    "if train_end >= test_start:\n",
    "    raise ValueError('Not enough history for requested split')\n",
    "\n",
    "train_mask = df.index < train_end\n",
    "val_mask = (df.index >= train_end) & (df.index < test_start)\n",
    "test_mask = df.index >= test_start\n",
    "\n",
    "df_train = df.loc[train_mask].copy()\n",
    "df_val = df.loc[val_mask].copy()\n",
    "df_test = df.loc[test_mask].copy()\n",
    "\n",
    "print('date range:', start.date(), '->', end.date())\n",
    "print('train:', df_train.index.min().date(), '->', df_train.index.max().date(), 'rows:', df_train.shape[0])\n",
    "print('val  :', df_val.index.min().date(), '->', df_val.index.max().date(), 'rows:', df_val.shape[0])\n",
    "print('test :', df_test.index.min().date(), '->', df_test.index.max().date(), 'rows:', df_test.shape[0])\n",
    "print('assets:', df['Asset_ID'].nunique())\n",
    "\n",
    "# Numeric feature columns\n",
    "exclude = {'Asset_ID', TARGET_FWD_COL}\n",
    "feature_cols = [c for c in df.columns if c not in exclude]\n",
    "numeric_feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "print('n_numeric_features:', len(numeric_feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9878c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dense tensors X[dates, assets, features] and r[dates, assets]\n",
    "\n",
    "N_FEATURES = 32\n",
    "LOOKBACK = 20  # per PDF window size\n",
    "\n",
    "# feature selection (MI) on training rows\n",
    "X_fs = df_train[numeric_feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "y_fs = df_train[TARGET_FWD_COL].astype(float)\n",
    "\n",
    "med = X_fs.median(axis=0)\n",
    "X_imp = X_fs.fillna(med)\n",
    "\n",
    "mi = mutual_info_classif(X_imp.to_numpy(), (y_fs.to_numpy() > 0).astype(int), random_state=42)\n",
    "mi_s = pd.Series(mi, index=numeric_feature_cols).sort_values(ascending=False)\n",
    "sel_features = mi_s.head(min(N_FEATURES, len(mi_s))).index.tolist()\n",
    "print('selected features:', sel_features)\n",
    "\n",
    "# Pivot per feature -> [T,N]\n",
    "dates = pd.Index(sorted(df.index.unique()))\n",
    "assets = pd.Index(sorted(df['Asset_ID'].unique()))\n",
    "\n",
    "feat_arrays = []\n",
    "for f in sel_features:\n",
    "    m = df.pivot_table(index=df.index, columns='Asset_ID', values=f, aggfunc='mean').reindex(index=dates, columns=assets)\n",
    "    feat_arrays.append(m.to_numpy(dtype=np.float32))\n",
    "\n",
    "X_all = np.stack(feat_arrays, axis=-1)  # [T,N,F]\n",
    "\n",
    "r_all = (\n",
    "    df.pivot_table(index=df.index, columns='Asset_ID', values=TARGET_FWD_COL, aggfunc='mean')\n",
    "    .reindex(index=dates, columns=assets)\n",
    "    .to_numpy(dtype=np.float32)\n",
    ")\n",
    "\n",
    "# Train normalization stats\n",
    "train_dates_mask = dates < train_end\n",
    "x_tr = X_all[train_dates_mask]\n",
    "med_f = np.nanmedian(x_tr, axis=(0, 1))\n",
    "X_all = np.where(np.isnan(X_all), med_f, X_all)\n",
    "mean_f = X_all[train_dates_mask].mean(axis=(0, 1))\n",
    "std_f = X_all[train_dates_mask].std(axis=(0, 1), ddof=0)\n",
    "std_f = np.where(std_f == 0.0, 1.0, std_f)\n",
    "X_all = (X_all - mean_f) / std_f\n",
    "\n",
    "# Index lists (prediction dates)\n",
    "train_idx = [i for i, dt in enumerate(dates) if (dt < train_end) and i >= LOOKBACK - 1]\n",
    "val_idx = [i for i, dt in enumerate(dates) if (dt >= df_val.index.min()) and (dt <= df_val.index.max()) and i >= LOOKBACK - 1]\n",
    "test_idx = [i for i, dt in enumerate(dates) if (dt >= df_test.index.min()) and (dt <= df_test.index.max()) and i >= LOOKBACK - 1]\n",
    "\n",
    "print('n_train_dates:', len(train_idx), 'n_val_dates:', len(val_idx), 'n_test_dates:', len(test_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167927f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior relationship D: long-term correlation on training window using close prices\n",
    "from src.backtester.data import load_cleaned_assets, align_close_prices\n",
    "\n",
    "assets_ohlcv = load_cleaned_assets(symbols=assets.to_list(), cleaned_dir=str(PROJECT_ROOT / 'dataset' / 'cleaned'))\n",
    "close_prices = align_close_prices(assets_ohlcv).sort_index().ffill().bfill()\n",
    "\n",
    "cp_train = close_prices.loc[:train_end].copy()\n",
    "ret = cp_train.pct_change().dropna(how='all')\n",
    "ret = ret.dropna(axis=1, how='any')\n",
    "\n",
    "# keep consistent ordering\n",
    "assets_bt = ret.columns.to_list()\n",
    "idx_assets = [assets.get_loc(a) for a in assets_bt]\n",
    "\n",
    "corr = ret.corr().fillna(0.0).to_numpy(dtype=np.float32)\n",
    "prior_d = torch.tensor(corr, device=device)\n",
    "print('prior_d:', prior_d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.finmamba import FinMamba, FinMambaConfig, finmamba_loss\n",
    "\n",
    "cfg = FinMambaConfig(lookback=LOOKBACK, n_levels=2, hidden_dim=64, n_heads=4)\n",
    "\n",
    "model = FinMamba(feature_dim=len(sel_features), cfg=cfg).to(device)\n",
    "model.set_prior(prior_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9845b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (epochs=3)\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_DATES = 8\n",
    "LR = 1e-3\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DateDataset(Dataset):\n",
    "    def __init__(self, idxs: list[int]):\n",
    "        self.idxs = idxs\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idxs)\n",
    "    def __getitem__(self, k: int):\n",
    "        i = self.idxs[k]\n",
    "        X_seq = X_all[i - LOOKBACK + 1 : i + 1][:, idx_assets, :]  # [L,N,F]\n",
    "        r = r_all[i][idx_assets]  # [N]\n",
    "        return (\n",
    "            torch.tensor(X_seq, dtype=torch.float32),\n",
    "            torch.tensor(r, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "train_loader = DataLoader(DateDataset(train_idx), batch_size=BATCH_DATES, shuffle=True)\n",
    "val_loader = DataLoader(DateDataset(val_idx), batch_size=BATCH_DATES, shuffle=False)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'epoch {epoch}/{EPOCHS} (train)')\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    for x_seq, r in train_loader:\n",
    "        x_seq = x_seq.to(device)  # [B,L,N,F]\n",
    "        r = r.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        y_pred, z_seq, s_seq = model(x_seq)\n",
    "        loss = finmamba_loss(y_pred=y_pred, r_true=r, z_seq=z_seq, s_seq=s_seq, cfg=cfg)\n",
    "        loss.backward()\n",
    "        train_losses.append(float(loss.item()))\n",
    "        opt.step()\n",
    "\n",
    "    # quick val loss\n",
    "    if train_losses:\n",
    "        print('  train_loss', float(np.mean(train_losses)))\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for b, (x_seq, r) in enumerate(val_loader):\n",
    "            if b >= 10:\n",
    "                break\n",
    "            x_seq = x_seq.to(device)\n",
    "            r = r.to(device)\n",
    "            y_pred, z_seq, s_seq = model(x_seq)\n",
    "            losses.append(float(finmamba_loss(y_pred=y_pred, r_true=r, z_seq=z_seq, s_seq=s_seq, cfg=cfg).item()))\n",
    "    if losses:\n",
    "        print('  val_loss', float(np.mean(losses)))\n",
    "    else:\n",
    "        print('  val_loss n/a (no validation batches)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict scores on test and run backtest + Bokeh\n",
    "\n",
    "from src.backtester.engine import BacktestConfig, run_backtest\n",
    "from src.backtester.report import compute_backtest_report\n",
    "from src.backtester.bokeh_plots import build_interactive_portfolio_layout\n",
    "from src.backtester.portfolio import equal_weight\n",
    "\n",
    "model.eval()\n",
    "all_dates = []\n",
    "all_assets = []\n",
    "all_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in test_idx:\n",
    "        x_seq = torch.tensor(X_all[i - LOOKBACK + 1 : i + 1][:, idx_assets, :], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        y_pred, _z, _s = model(x_seq)\n",
    "        score = y_pred.squeeze(0).cpu().numpy()  # [N]\n",
    "\n",
    "        dt = pd.Timestamp(dates[i])\n",
    "        all_dates.append(np.repeat(np.datetime64(dt.to_datetime64()), len(assets_bt)))\n",
    "        all_assets.append(np.array(assets_bt, dtype=object))\n",
    "        all_scores.append(score)\n",
    "\n",
    "dates_test = np.concatenate(all_dates)\n",
    "assets_test = np.concatenate(all_assets)\n",
    "scores_test = np.concatenate(all_scores)\n",
    "\n",
    "long = pd.DataFrame({'Date': dates_test, 'Asset_ID': assets_test, 'score': scores_test})\n",
    "pred_matrix = long.pivot_table(index='Date', columns='Asset_ID', values='score', aggfunc='mean').sort_index()\n",
    "\n",
    "# Slice backtest window to test period\n",
    "bt_start = pd.Timestamp(df_test.index.min())\n",
    "bt_end = pd.Timestamp(df_test.index.max())\n",
    "close_bt = close_prices.loc[bt_start:bt_end, assets_bt]\n",
    "\n",
    "market_df = pd.DataFrame({\n",
    "    'Open': pd.concat([d['Open'] for d in assets_ohlcv.values()], axis=1).mean(axis=1).loc[bt_start:bt_end],\n",
    "    'High': pd.concat([d['High'] for d in assets_ohlcv.values()], axis=1).mean(axis=1).loc[bt_start:bt_end],\n",
    "    'Low': pd.concat([d['Low'] for d in assets_ohlcv.values()], axis=1).mean(axis=1).loc[bt_start:bt_end],\n",
    "    'Close': pd.concat([d['Close'] for d in assets_ohlcv.values()], axis=1).mean(axis=1).loc[bt_start:bt_end],\n",
    "    'Volume': pd.concat([d['Volume'] for d in assets_ohlcv.values()], axis=1).sum(axis=1).loc[bt_start:bt_end],\n",
    "}).sort_index()\n",
    "\n",
    "pred_matrix = pred_matrix.reindex(close_bt.index)\n",
    "\n",
    "# Weekly Top-K long-only weights\n",
    "REBALANCE_FREQ = 'W'\n",
    "TOP_K = 20\n",
    "\n",
    "rebal_dates = set(pd.Series(pred_matrix.index, index=pred_matrix.index).resample(REBALANCE_FREQ).last().dropna().tolist())\n",
    "\n",
    "w_last = pd.Series(0.0, index=assets_bt)\n",
    "w_rows = []\n",
    "for dt in pred_matrix.index:\n",
    "    if dt in rebal_dates:\n",
    "        row = pred_matrix.loc[dt].dropna().sort_values(ascending=False)\n",
    "        top = row.head(min(TOP_K, len(row)))\n",
    "        candidates = [a for a, v in top.items() if np.isfinite(v)]\n",
    "        if len(candidates) == 0:\n",
    "            w_last = pd.Series(0.0, index=assets_bt)\n",
    "        else:\n",
    "            w_dict = equal_weight(candidates)\n",
    "            w_last = pd.Series(0.0, index=assets_bt)\n",
    "            for a, w in w_dict.items():\n",
    "                if a in w_last.index:\n",
    "                    w_last[a] = float(w)\n",
    "    w_rows.append(w_last)\n",
    "\n",
    "weights = pd.DataFrame(w_rows, index=pred_matrix.index, columns=assets_bt).fillna(0.0)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "bt_cfg = BacktestConfig(initial_equity=1_000_000.0, transaction_cost_bps=5.0, mode='vectorized')\n",
    "res = run_backtest(close_bt, weights, config=bt_cfg)\n",
    "report = compute_backtest_report(result=res, close_prices=close_bt)\n",
    "display(report.to_frame('FinMamba Report'))\n",
    "\n",
    "layout = build_interactive_portfolio_layout(\n",
    "    market_ohlcv=market_df,\n",
    "    equity=res.equity,\n",
    "    returns=res.returns,\n",
    "    weights=res.weights,\n",
    "    turnover=res.turnover,\n",
    "    costs=res.costs,\n",
    "    close_prices=close_bt,\n",
    "    title='FinMamba (PDF-inspired) - Backtest',\n",
    ")\n",
    "show(layout)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
