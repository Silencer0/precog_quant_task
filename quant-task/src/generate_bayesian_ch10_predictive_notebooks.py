from __future__ import annotations

"""Generate Chapter 10 (Bayesian ML) predictive-model notebooks.

These notebooks live next to the original chapter materials under:
`ml_finance_thoery/machine-learning-for-trading/10_bayesian_machine_learning/predictive_models/`.

They intentionally avoid PyMC/PyMC3 because those libraries are not available in the
Python 3.13 environment in this repository.
"""

from dataclasses import dataclass
from pathlib import Path

import nbformat as nbf


@dataclass(frozen=True)
class NotebookSpec:
    filename: str
    title: str
    intro_md: str
    cells: list[nbf.NotebookNode]


def _md(text: str) -> nbf.NotebookNode:
    return nbf.v4.new_markdown_cell(text)


def _code(code: str) -> nbf.NotebookNode:
    return nbf.v4.new_code_cell(code)


def _common_preamble(*, method_name: str) -> list[nbf.NotebookNode]:
    # Keep this preamble lightweight and self-contained so each notebook runs in
    # isolation from any working directory.
    return [
        _md(
            f"# Chapter 10 Bayesian ML (Predictive): {method_name}\n\n"
            "These notebooks mirror the *methods* highlighted in\n"
            "`ml_finance_thoery/machine-learning-for-trading/10_bayesian_machine_learning/README.md`\n"
            "and apply them to the local `dataset/cleaned/` asset universe to produce\n"
            "out-of-sample predictions and a backtest using the same **vectorized** engine\n"
            "used by the `notebooks/ML_Linear_Models_*` notebooks.\n"
        ),
        _code(
            "from __future__ import annotations\n\n"
            "import math\n"
            "from pathlib import Path\n"
            "import sys\n\n"
            "import numpy as np\n"
            "import pandas as pd\n\n"
            "SEED = 42\n"
            "rng = np.random.default_rng(SEED)\n\n"
            "def find_project_root(start: Path) -> Path:\n"
            "    p = start.resolve()\n"
            "    for _ in range(10):\n"
            "        if (p / 'src').exists() and (p / 'dataset').exists():\n"
            "            return p\n"
            "        p = p.parent\n"
            "    raise RuntimeError(f'Could not find project root from: {start!s}')\n\n"
            "PROJECT_ROOT = find_project_root(Path.cwd())\n"
            "CLEANED_DIR = PROJECT_ROOT / 'dataset' / 'cleaned'\n\n"
            "# Ensure `src/` is on sys.path so `backtester` is importable\n"
            "src_dir = PROJECT_ROOT / 'src'\n"
            "if str(src_dir) not in sys.path:\n"
            "    sys.path.append(str(src_dir))\n"
        ),
    ]


def _backtest_cell(*, title: str) -> nbf.NotebookNode:
    # This follows the same approach as in `notebooks/ML_Linear_Models_05_SMC_Indicators_TimeSplit.ipynb`:
    # 1) build weights from a prediction matrix (date x asset)
    # 2) run `run_backtest` in vectorized mode
    # 3) render report + Bokeh dashboard
    return _code(
        "from IPython.display import display\n"
        "from bokeh.io import output_notebook, show\n\n"
        "from backtester.data import load_cleaned_assets, align_close_prices\n"
        "from backtester.engine import BacktestConfig, run_backtest\n"
        "from backtester.report import compute_backtest_report\n"
        "from backtester.bokeh_plots import build_interactive_portfolio_layout\n"
        "from backtester.portfolio import equal_weight, optimize_mpt\n\n"
        "output_notebook()\n\n"
        "if 'pred_matrix' not in globals():\n"
        "    raise RuntimeError('Expected `pred_matrix` (index=date, columns=Asset_ID) to exist')\n\n"
        "# IMPORTANT: performance metrics should start when the model has signals.\n"
        "# Keep the original prediction date range before reindexing to market data.\n"
        "pred_range = pd.DatetimeIndex(pred_matrix.index).sort_values()\n"
        "if pred_range.empty:\n"
        "    raise RuntimeError('pred_matrix has empty index')\n"
        "bt_start = pd.Timestamp(pred_range[0])\n"
        "bt_end = pd.Timestamp(pred_range[-1])\n\n"
        "bt_assets = sorted([str(c) for c in pred_matrix.columns.tolist()])\n"
        "assets_ohlcv = load_cleaned_assets(symbols=bt_assets, cleaned_dir=str(CLEANED_DIR))\n"
        "close_prices = align_close_prices(assets_ohlcv)\n\n"
        "# Align prediction matrix to available trade dates\n"
        "pred_matrix = pred_matrix.reindex(close_prices.index)\n"
        "# Restrict backtest to the prediction window (avoid 2016-start metrics).\n"
        "close_prices = close_prices.loc[bt_start:bt_end]\n"
        "pred_matrix = pred_matrix.loc[bt_start:bt_end]\n"
        "returns_matrix = close_prices.pct_change().fillna(0.0)\n\n"
        "market_df = pd.DataFrame({\n"
        "    'Open': pd.concat([df['Open'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n"
        "    'High': pd.concat([df['High'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n"
        "    'Low': pd.concat([df['Low'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n"
        "    'Close': pd.concat([df['Close'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n"
        "    'Volume': pd.concat([df['Volume'] for df in assets_ohlcv.values()], axis=1).sum(axis=1),\n"
        "}).sort_index().loc[bt_start:bt_end]\n\n"
        "REBALANCE_FREQ = 'W'\n"
        "TOP_K = min(20, len(bt_assets))\n"
        "LOOKBACK_DAYS = 126\n\n"
        "def build_weights_from_predictions(pred_matrix: pd.DataFrame, *, pm_style: str) -> pd.DataFrame:\n"
        "    rebal_dates = set(pd.Series(pred_matrix.index, index=pred_matrix.index).resample(REBALANCE_FREQ).last().dropna().tolist())\n"
        "    w_last = pd.Series(0.0, index=bt_assets)\n"
        "    rows = []\n"
        "    for dt in pred_matrix.index:\n"
        "        if dt in rebal_dates:\n"
        "            row = pred_matrix.loc[dt].dropna().sort_values(ascending=False)\n"
        "            top = row.head(TOP_K)\n"
        "            candidates = [a for a, v in top.items() if np.isfinite(v) and float(v) > 0.0]\n"
        "            if not candidates:\n"
        "                w_last = pd.Series(0.0, index=bt_assets)\n"
        "            else:\n"
        "                if pm_style == '1N':\n"
        "                    w_dict = equal_weight(candidates)\n"
        "                elif pm_style == 'MPT':\n"
        "                    w_dict = optimize_mpt(returns_matrix, candidates, dt, lookback_days=LOOKBACK_DAYS)\n"
        "                else:\n"
        "                    raise ValueError(f'Unknown pm_style: {pm_style!r}')\n"
        "                w_last = pd.Series(0.0, index=bt_assets)\n"
        "                for a, w in w_dict.items():\n"
        "                    w_last[str(a)] = float(w)\n"
        "        rows.append(w_last)\n"
        "    return pd.DataFrame(rows, index=pred_matrix.index, columns=bt_assets).fillna(0.0)\n\n"
        "cfg = BacktestConfig(initial_equity=1_000_000.0, transaction_cost_bps=5.0, mode='vectorized')\n\n"
        "compare_rows = []\n"
        "results = {}\n"
        "for pm_style in ['1N', 'MPT']:\n"
        "    w = build_weights_from_predictions(pred_matrix, pm_style=pm_style)\n"
        "    res = run_backtest(close_prices, w, config=cfg)\n"
        "    rpt = compute_backtest_report(result=res, close_prices=close_prices)\n"
        "    results[pm_style] = (w, res, rpt)\n"
        "    compare_rows.append({\n"
        "        'style': pm_style,\n"
        "        'Total Return [%]': float(rpt['Total Return [%]']),\n"
        "        'CAGR [%]': float(rpt['CAGR [%]']),\n"
        "        'Sharpe': float(rpt['Sharpe']),\n"
        "        'Max Drawdown [%]': float(rpt['Max Drawdown [%]']),\n"
        "    })\n"
        "compare = pd.DataFrame(compare_rows).sort_values('Total Return [%]', ascending=False).reset_index(drop=True)\n"
        "display(compare)\n\n"
        "BASE_TITLE = " + repr(title) + "\n"
        "for pm_style in ['1N', 'MPT']:\n"
        "    w, res, rpt = results[pm_style]\n"
        "    title = BASE_TITLE + ' - ' + pm_style\n"
        "    display(rpt.to_frame(title))\n"
        "    layout = build_interactive_portfolio_layout(\n"
        "        market_ohlcv=market_df,\n"
        "        equity=res.equity,\n"
        "        returns=res.returns,\n"
        "        weights=res.weights,\n"
        "        turnover=res.turnover,\n"
        "        costs=res.costs,\n"
        "        close_prices=close_prices,\n"
        "        title=title,\n"
        "    )\n"
        "    show(layout)\n"
    )


def _nb(*, spec: NotebookSpec) -> nbf.NotebookNode:
    nb = nbf.v4.new_notebook()
    nb["metadata"] = {
        "kernelspec": {
            "display_name": ".venv_uv",
            "language": "python",
            "name": "python3",
        },
        "language_info": {"name": "python", "version": "3.13"},
    }
    nb.cells = [_md(spec.title), _md(spec.intro_md), *spec.cells]
    return nb


def _write(nb: nbf.NotebookNode, path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        nbf.write(nb, f)


def create_notebooks(*, out_dir: Path) -> list[Path]:
    specs: list[NotebookSpec] = []

    # 1) Conjugate priors (Beta-Binomial) for direction prediction
    specs.append(
        NotebookSpec(
            filename="01_conjugate_priors_beta_binomial_direction.ipynb",
            title="# Bayesian Updating with Conjugate Priors (Beta-Binomial)",
            intro_md=(
                "We model next-day return **direction** as a Bernoulli random variable.\n\n"
                "- Observation: $y_t = 1$ if $r_{t+1} > 0$, else $0$\n"
                "- Likelihood: $y_t \\sim \\text{Bernoulli}(p)$\n"
                "- Prior: $p \\sim \\text{Beta}(\\alpha_0, \\beta_0)$\n"
                "- Posterior: $p | y_{1:n} \\sim \\text{Beta}(\\alpha_0 + \\sum y_i, \\beta_0 + n - \\sum y_i)$\n\n"
                "The posterior mean $\\mathbb{E}[p|\\cdot]$ becomes a probability forecast and a trading signal.\n"
            ),
            cells=[
                *_common_preamble(method_name="Conjugate Priors"),
                _code(
                    "# Load close prices for all assets\n"
                    "from backtester.data import load_cleaned_assets, align_close_prices\n\n"
                    "files = sorted([p for p in CLEANED_DIR.glob('Asset_*.csv')])\n"
                    "symbols = [p.stem for p in files]\n"
                    "assets_ohlcv = load_cleaned_assets(symbols=symbols, cleaned_dir=str(CLEANED_DIR))\n"
                    "close_prices = align_close_prices(assets_ohlcv).sort_index()\n\n"
                    "rets = close_prices.pct_change()\n"
                    "y_up_fwd = (rets.shift(-1) > 0).astype(int)\n\n"
                    "# Time split: align boundaries like ML_linear_models notebooks\n"
                    "TRAIN_YEARS = 7\nVAL_MONTHS = 18\nTEST_MONTHS = 18\n\n"
                    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n"
                    "    pos = int(index.searchsorted(ts, side='left'))\n"
                    "    if pos >= len(index):\n"
                    "        return pd.Timestamp(index[-1])\n"
                    "    return pd.Timestamp(index[pos])\n\n"
                    "idx = pd.DatetimeIndex(close_prices.index).sort_values()\n"
                    "end = pd.Timestamp(idx[-1])\n"
                    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n"
                    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n"
                    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n"
                    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n"
                    "val_start = align_to_trading_date(idx, pd.Timestamp(raw_val_start))\n"
                    "train_start = align_to_trading_date(idx, pd.Timestamp(raw_train_start))\n"
                    "print('aligned boundaries:', train_start, val_start, test_start, end)\n\n"
                    "# Conjugate updating (rolling window to stay adaptive)\n"
                    "ALPHA0 = 1.0\nBETA0 = 1.0\nLOOKBACK_DAYS = 252\n\n"
                    "# Posterior mean estimate per day/asset\n"
                    "p_up = pd.DataFrame(index=close_prices.index, columns=close_prices.columns, dtype=float)\n"
                    "for a in close_prices.columns:\n"
                    "    y = y_up_fwd[a].dropna().astype(int)\n"
                    "    # For each date, count successes in the trailing window ending at t-1\n"
                    "    for t in range(len(y.index)):\n"
                    "        dt = y.index[t]\n"
                    "        start = max(0, t - LOOKBACK_DAYS)\n"
                    "        window = y.iloc[start:t]  # exclude current day to avoid leakage\n"
                    "        s = int(window.sum())\n"
                    "        n = int(window.shape[0])\n"
                    "        a_post = ALPHA0 + s\n"
                    "        b_post = BETA0 + (n - s)\n"
                    "        p_up.loc[dt, a] = a_post / (a_post + b_post)\n\n"
                    "# Signal for backtest: center around 0\n"
                    "signal = (p_up - 0.5).loc[test_start:end]\n"
                    "pred_matrix = signal.dropna(how='all').fillna(0.0)\n"
                ),
                _md(
                    "## Backtest\n"
                    "We reuse the same *vectorized* backtest engine and portfolio construction\n"
                    "style as the `notebooks/ML_Linear_Models_*` notebooks:\n\n"
                    "- weekly rebalance\n"
                    "- select top-K assets by prediction (must be positive)\n"
                    "- 1/N equal-weight and MPT variants\n"
                ),
                _backtest_cell(title="Bayes Conjugate Priors (Beta-Binomial)"),
            ],
        )
    )

    # 2) Bayesian logistic regression (MAP + Laplace)
    specs.append(
        NotebookSpec(
            filename="02_bayesian_logistic_regression_map_laplace.ipynb",
            title="# Bayesian Logistic Regression (MAP + Laplace Approximation)",
            intro_md=(
                "We model $y \\in \\{0,1\\}$ (next-day up move) using logistic regression with a Gaussian prior.\n\n"
                "- Likelihood: $p(y=1|x,\\\\beta)=\\\\sigma(x^T\\\\beta)$\n"
                "- Prior: $\\\\beta \\sim \\mathcal{N}(0, \\tau^2 I)$\n"
                "- MAP: maximize $\\\\log p(\\\\beta|D) = \\sum_i \\log p(y_i|x_i,\\\\beta) - \\frac{1}{2\\\\tau^2}\\\\|\\\\beta\\\\|^2$\n\n"
                "Laplace approximation uses the Hessian at the MAP estimate to approximate\n"
                "$p(\\beta|D)$ as Gaussian.\n"
            ),
            cells=[
                *_common_preamble(method_name="Bayesian Logistic Regression (MAP)"),
                _code(
                    "from dataclasses import dataclass\n"
                    "from scipy.optimize import minimize\n"
                    "from sklearn.impute import SimpleImputer\n"
                    "from sklearn.preprocessing import StandardScaler\n\n"
                    "# Feature set: reuse the same SMC + OHLCV engineering pattern from ML_linear_models_05\n"
                    "# but keep it in-notebook for portability.\n\n"
                    "SWING_WINDOW = 5\nN_CONSEC_SWINGS = 3\n\n"
                    "def compute_features(ohlcv: pd.DataFrame) -> pd.DataFrame:\n"
                    "    df = ohlcv.copy().sort_index()\n"
                    "    o = df['Open'].astype(float)\n"
                    "    h = df['High'].astype(float)\n"
                    "    l = df['Low'].astype(float)\n"
                    "    c = df['Close'].astype(float)\n"
                    "    v = df['Volume'].astype(float)\n"
                    "    ret_1d = c.pct_change()\n"
                    "    hl_range = (h - l)\n"
                    "    hl_range_pct = hl_range / c.replace(0.0, np.nan)\n"
                    "    body = (c - o)\n"
                    "    body_pct = body / c.replace(0.0, np.nan)\n"
                    "    hlc3 = (h + l + c) / 3.0\n"
                    "    vol_z_20 = (v - v.rolling(20).mean()) / v.rolling(20).std()\n\n"
                    "    w = int(SWING_WINDOW)\n"
                    "    win = 2 * w + 1\n"
                    "    swing_high = h.eq(h.rolling(win, center=True, min_periods=win).max())\n"
                    "    swing_low = l.eq(l.rolling(win, center=True, min_periods=win).min())\n"
                    "    last_swing_high = h.where(swing_high).ffill()\n"
                    "    last_swing_low = l.where(swing_low).ffill()\n"
                    "    prev_high = last_swing_high.shift(1)\n"
                    "    prev_low = last_swing_low.shift(1)\n"
                    "    bos_bull = (c > prev_high) & (c.shift(1) <= prev_high) & prev_high.notna()\n"
                    "    bos_bear = (c < prev_low) & (c.shift(1) >= prev_low) & prev_low.notna()\n\n"
                    "    # Simplified CHOCH proxy (trend not explicitly inferred here)\n"
                    "    choch_bull = bos_bull.astype(int)\n"
                    "    choch_bear = bos_bear.astype(int)\n\n"
                    "    out = pd.DataFrame({\n"
                    "        'open': o,\n"
                    "        'high': h,\n"
                    "        'low': l,\n"
                    "        'close': c,\n"
                    "        'volume': v,\n"
                    "        'ret_1d': ret_1d,\n"
                    "        'hl_range': hl_range,\n"
                    "        'hl_range_pct': hl_range_pct,\n"
                    "        'body': body,\n"
                    "        'body_pct': body_pct,\n"
                    "        'hlc3': hlc3,\n"
                    "        'vol_z_20': vol_z_20,\n"
                    "        'bos_bull': bos_bull.astype(int),\n"
                    "        'bos_bear': bos_bear.astype(int),\n"
                    "        'choch_bull': choch_bull,\n"
                    "        'choch_bear': choch_bear,\n"
                    "    }).sort_index()\n"
                    "    out['ret_1d_fwd'] = out['ret_1d'].shift(-1)\n"
                    "    out['y_up_fwd'] = (out['ret_1d_fwd'] > 0).astype(int)\n"
                    "    return out\n\n"
                    "# Load data and build a long panel (Date index, Asset_ID column)\n"
                    "from backtester.data import load_cleaned_assets\n\n"
                    "files = sorted([p for p in CLEANED_DIR.glob('Asset_*.csv')])\n"
                    "symbols = [p.stem for p in files]\n"
                    "assets_ohlcv = load_cleaned_assets(symbols=symbols, cleaned_dir=str(CLEANED_DIR))\n\n"
                    "frames = []\n"
                    "for sym, df in assets_ohlcv.items():\n"
                    "    feat = compute_features(df)\n"
                    "    feat['Asset_ID'] = sym\n"
                    "    frames.append(feat)\n\n"
                    "data = pd.concat(frames, axis=0).sort_index()\n"
                    "data = data.dropna(subset=['y_up_fwd'])\n\n"
                    "# Time split (same policy as ML_linear_models notebooks)\n"
                    "TRAIN_YEARS = 7\nVAL_MONTHS = 18\nTEST_MONTHS = 18\n\n"
                    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n"
                    "    pos = int(index.searchsorted(ts, side='left'))\n"
                    "    if pos >= len(index):\n"
                    "        return pd.Timestamp(index[-1])\n"
                    "    return pd.Timestamp(index[pos])\n\n"
                    "idx = pd.DatetimeIndex(data.index.unique()).sort_values()\n"
                    "end = pd.Timestamp(idx[-1])\n"
                    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n"
                    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n"
                    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n"
                    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n"
                    "val_start = align_to_trading_date(idx, pd.Timestamp(raw_val_start))\n"
                    "train_start = align_to_trading_date(idx, pd.Timestamp(raw_train_start))\n"
                    "print('aligned boundaries:', train_start, val_start, test_start, end)\n\n"
                    "df_train = data.loc[(data.index >= train_start) & (data.index < val_start)].copy()\n"
                    "df_val = data.loc[(data.index >= val_start) & (data.index < test_start)].copy()\n"
                    "df_test = data.loc[(data.index >= test_start) & (data.index <= end)].copy()\n\n"
                    "feature_cols = [c for c in df_train.columns if c not in {'Asset_ID', 'ret_1d_fwd', 'y_up_fwd'} and pd.api.types.is_numeric_dtype(df_train[c])]\n"
                    "X_train = df_train[feature_cols].replace([np.inf, -np.inf], np.nan)\n"
                    "y_train = df_train['y_up_fwd'].astype(int).to_numpy()\n"
                    "X_test = df_test[feature_cols].replace([np.inf, -np.inf], np.nan)\n\n"
                    "# Preprocess (impute + standardize)\n"
                    "imp = SimpleImputer(strategy='median')\n"
                    "scaler = StandardScaler(with_mean=True, with_std=True)\n"
                    "X_train_imp = imp.fit_transform(X_train)\n"
                    "X_train_std = scaler.fit_transform(X_train_imp)\n"
                    "X_test_std = scaler.transform(imp.transform(X_test))\n\n"
                    "def sigmoid(z: np.ndarray) -> np.ndarray:\n"
                    "    return 1.0 / (1.0 + np.exp(-z))\n\n"
                    "# Add intercept\n"
                    "Xtr = np.hstack([np.ones((X_train_std.shape[0], 1)), X_train_std])\n"
                    "Xte = np.hstack([np.ones((X_test_std.shape[0], 1)), X_test_std])\n\n"
                    "TAU2 = 10.0**2  # prior variance\n\n"
                    "def neg_log_posterior(beta: np.ndarray) -> float:\n"
                    "    z = Xtr @ beta\n"
                    "    p = sigmoid(z)\n"
                    "    eps = 1e-12\n"
                    "    ll = np.sum(y_train * np.log(p + eps) + (1 - y_train) * np.log(1 - p + eps))\n"
                    "    lp = -0.5 * np.sum(beta[1:] ** 2) / TAU2  # no prior on intercept\n"
                    "    return float(-(ll + lp))\n\n"
                    "def grad_neg_log_posterior(beta: np.ndarray) -> np.ndarray:\n"
                    "    z = Xtr @ beta\n"
                    "    p = sigmoid(z)\n"
                    "    g = Xtr.T @ (p - y_train)\n"
                    "    g[1:] += beta[1:] / TAU2\n"
                    "    return g\n\n"
                    "beta0 = np.zeros(Xtr.shape[1])\n"
                    "res = minimize(neg_log_posterior, beta0, jac=grad_neg_log_posterior, method='BFGS')\n"
                    "if not res.success:\n"
                    "    raise RuntimeError(res.message)\n"
                    "beta_map = res.x\n"
                    "cov_laplace = res.hess_inv\n"
                    "print('MAP beta shape:', beta_map.shape)\n\n"
                    "# Posterior predictive via Monte Carlo from Laplace approximation\n"
                    "S = 200\n"
                    "beta_samps = rng.multivariate_normal(mean=beta_map, cov=cov_laplace, size=S)\n"
                    "p_samps = sigmoid(Xte @ beta_samps.T)\n"
                    "p_mean = p_samps.mean(axis=1)\n\n"
                    "# Convert to per-date, per-asset prediction matrix\n"
                    "pred_long = pd.DataFrame({\n"
                    "    'Date': df_test.index,\n"
                    "    'Asset_ID': df_test['Asset_ID'].to_numpy(),\n"
                    "    'p_up': p_mean,\n"
                    "})\n"
                    "pred_long['signal'] = pred_long['p_up'] - 0.5\n"
                    "pred_matrix = pred_long.pivot_table(index='Date', columns='Asset_ID', values='signal', aggfunc='mean').sort_index()\n"
                ),
                _backtest_cell(title="Bayes Logistic Regression (MAP+Laplace)"),
            ],
        )
    )

    # 3) MCMC: Metropolis-Hastings (subsampled)
    specs.append(
        NotebookSpec(
            filename="03_bayesian_logistic_regression_mcmc_metropolis.ipynb",
            title="# Bayesian Logistic Regression (MCMC: Metropolis-Hastings)",
            intro_md=(
                "This notebook uses **random-walk Metropolis-Hastings** to sample from the\n"
                "posterior of logistic regression coefficients.\n\n"
                "Because MCMC cost scales poorly with dataset size, we intentionally run on a\n"
                "smaller universe and/or a subsample of observations.\n"
            ),
            cells=[
                *_common_preamble(
                    method_name="Bayesian Logistic Regression (MCMC: MH)"
                ),
                _code(
                    "from scipy.special import expit\n"
                    "from sklearn.impute import SimpleImputer\n"
                    "from sklearn.preprocessing import StandardScaler\n\n"
                    "ASSET_LIMIT = 20\n"
                    "N_TRAIN_SAMPLES = 20_000\n"
                    "N_STEPS = 4_000\n"
                    "BURN = 1_000\n"
                    "STEP_SCALE = 0.05\n"
                    "TAU2 = 10.0**2\n\n"
                    "# Reuse the same feature builder as MAP notebook by importing it from that notebook is not possible.\n"
                    "# Keep a minimal feature set for speed: returns + range + volume z-score.\n\n"
                    "def compute_features_fast(ohlcv: pd.DataFrame) -> pd.DataFrame:\n"
                    "    df = ohlcv.copy().sort_index()\n"
                    "    c = df['Close'].astype(float)\n"
                    "    h = df['High'].astype(float)\n"
                    "    l = df['Low'].astype(float)\n"
                    "    v = df['Volume'].astype(float)\n"
                    "    ret_1d = c.pct_change()\n"
                    "    hl_range_pct = (h - l) / c.replace(0.0, np.nan)\n"
                    "    vol_z_20 = (v - v.rolling(20).mean()) / v.rolling(20).std()\n"
                    "    out = pd.DataFrame({'ret_1d': ret_1d, 'hl_range_pct': hl_range_pct, 'vol_z_20': vol_z_20}).sort_index()\n"
                    "    out['ret_1d_fwd'] = out['ret_1d'].shift(-1)\n"
                    "    out['y_up_fwd'] = (out['ret_1d_fwd'] > 0).astype(int)\n"
                    "    return out\n\n"
                    "from backtester.data import load_cleaned_assets\n"
                    "files = sorted([p for p in CLEANED_DIR.glob('Asset_*.csv')])\n"
                    "symbols = [p.stem for p in files][:ASSET_LIMIT]\n"
                    "assets_ohlcv = load_cleaned_assets(symbols=symbols, cleaned_dir=str(CLEANED_DIR))\n\n"
                    "frames = []\n"
                    "for sym, df in assets_ohlcv.items():\n"
                    "    feat = compute_features_fast(df)\n"
                    "    feat['Asset_ID'] = sym\n"
                    "    frames.append(feat)\n"
                    "data = pd.concat(frames, axis=0).sort_index().dropna(subset=['y_up_fwd'])\n\n"
                    "# Time split\n"
                    "TRAIN_YEARS = 7\nVAL_MONTHS = 18\nTEST_MONTHS = 18\n\n"
                    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n"
                    "    pos = int(index.searchsorted(ts, side='left'))\n"
                    "    if pos >= len(index):\n"
                    "        return pd.Timestamp(index[-1])\n"
                    "    return pd.Timestamp(index[pos])\n\n"
                    "idx = pd.DatetimeIndex(data.index.unique()).sort_values()\n"
                    "end = pd.Timestamp(idx[-1])\n"
                    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n"
                    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n"
                    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n"
                    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n"
                    "val_start = align_to_trading_date(idx, pd.Timestamp(raw_val_start))\n"
                    "train_start = align_to_trading_date(idx, pd.Timestamp(raw_train_start))\n\n"
                    "df_train = data.loc[(data.index >= train_start) & (data.index < val_start)].copy()\n"
                    "df_test = data.loc[(data.index >= test_start) & (data.index <= end)].copy()\n\n"
                    "feature_cols = [c for c in df_train.columns if c not in {'Asset_ID', 'ret_1d_fwd', 'y_up_fwd'}]\n"
                    "X_train = df_train[feature_cols].replace([np.inf, -np.inf], np.nan)\n"
                    "y_train = df_train['y_up_fwd'].astype(int).to_numpy()\n"
                    "X_test = df_test[feature_cols].replace([np.inf, -np.inf], np.nan)\n\n"
                    "imp = SimpleImputer(strategy='median')\n"
                    "scaler = StandardScaler()\n"
                    "Xtr = scaler.fit_transform(imp.fit_transform(X_train))\n"
                    "Xte = scaler.transform(imp.transform(X_test))\n"
                    "Xtr = np.hstack([np.ones((Xtr.shape[0], 1)), Xtr])\n"
                    "Xte = np.hstack([np.ones((Xte.shape[0], 1)), Xte])\n\n"
                    "# Subsample training rows for MCMC\n"
                    "if Xtr.shape[0] > N_TRAIN_SAMPLES:\n"
                    "    idx_s = rng.choice(Xtr.shape[0], size=N_TRAIN_SAMPLES, replace=False)\n"
                    "    Xtr_s = Xtr[idx_s]\n"
                    "    y_s = y_train[idx_s]\n"
                    "else:\n"
                    "    Xtr_s = Xtr\n"
                    "    y_s = y_train\n\n"
                    "def log_post(beta: np.ndarray) -> float:\n"
                    "    z = Xtr_s @ beta\n"
                    "    p = expit(z)\n"
                    "    eps = 1e-12\n"
                    "    ll = float(np.sum(y_s * np.log(p + eps) + (1 - y_s) * np.log(1 - p + eps)))\n"
                    "    lp = float(-0.5 * np.sum(beta[1:] ** 2) / TAU2)\n"
                    "    return ll + lp\n\n"
                    "beta = np.zeros(Xtr_s.shape[1])\n"
                    "lp = log_post(beta)\n"
                    "samples = []\n"
                    "accept = 0\n"
                    "for t in range(N_STEPS):\n"
                    "    prop = beta + rng.normal(0.0, STEP_SCALE, size=beta.shape)\n"
                    "    lp_prop = log_post(prop)\n"
                    "    if math.log(rng.random()) < (lp_prop - lp):\n"
                    "        beta, lp = prop, lp_prop\n"
                    "        accept += 1\n"
                    "    if t >= BURN:\n"
                    "        samples.append(beta.copy())\n"
                    "print('accept_rate:', accept / N_STEPS)\n"
                    "B = np.stack(samples, axis=0)\n\n"
                    "# Posterior predictive probabilities\n"
                    "p_samps = expit(Xte @ B.T)\n"
                    "p_mean = p_samps.mean(axis=1)\n\n"
                    "pred_long = pd.DataFrame({'Date': df_test.index, 'Asset_ID': df_test['Asset_ID'].to_numpy(), 'signal': p_mean - 0.5})\n"
                    "pred_matrix = pred_long.pivot_table(index='Date', columns='Asset_ID', values='signal', aggfunc='mean').sort_index()\n"
                ),
                _backtest_cell(title="Bayes Logistic Regression (MH MCMC)"),
            ],
        )
    )

    # 4) Variational inference (mean-field Gaussian)
    specs.append(
        NotebookSpec(
            filename="04_bayesian_logistic_regression_variational_inference.ipynb",
            title="# Bayesian Logistic Regression (Variational Inference: Mean-Field Gaussian)",
            intro_md=(
                "Variational inference approximates the posterior $p(\\beta|D)$ with a tractable\n"
                "distribution $q_\\lambda(\\beta)$ by maximizing the ELBO:\n\n"
                "$$\\mathcal{L}(\\lambda) = \\mathbb{E}_{q_\\lambda}[\\log p(D,\\beta) - \\log q_\\lambda(\\beta)]$$\n\n"
                "We use a diagonal Gaussian $q(\\beta)=\\mathcal{N}(\\mu, \\text{diag}(\\sigma^2))$\n"
                "and optimize using stochastic gradients (reparameterization trick).\n"
            ),
            cells=[
                *_common_preamble(method_name="Bayesian Logistic Regression (VI)"),
                _code(
                    "from scipy.special import expit\n"
                    "from sklearn.impute import SimpleImputer\n"
                    "from sklearn.preprocessing import StandardScaler\n\n"
                    "ASSET_LIMIT = 50\n"
                    "N_TRAIN_SAMPLES = 80_000\n"
                    "N_STEPS = 1500\n"
                    "BATCH = 4096\n"
                    "LR = 1e-2\n"
                    "TAU2 = 10.0**2\n\n"
                    "def compute_features_fast(ohlcv: pd.DataFrame) -> pd.DataFrame:\n"
                    "    df = ohlcv.copy().sort_index()\n"
                    "    c = df['Close'].astype(float)\n"
                    "    h = df['High'].astype(float)\n"
                    "    l = df['Low'].astype(float)\n"
                    "    v = df['Volume'].astype(float)\n"
                    "    ret_1d = c.pct_change()\n"
                    "    hl_range_pct = (h - l) / c.replace(0.0, np.nan)\n"
                    "    vol_z_20 = (v - v.rolling(20).mean()) / v.rolling(20).std()\n"
                    "    out = pd.DataFrame({'ret_1d': ret_1d, 'hl_range_pct': hl_range_pct, 'vol_z_20': vol_z_20}).sort_index()\n"
                    "    out['ret_1d_fwd'] = out['ret_1d'].shift(-1)\n"
                    "    out['y_up_fwd'] = (out['ret_1d_fwd'] > 0).astype(int)\n"
                    "    return out\n\n"
                    "from backtester.data import load_cleaned_assets\n"
                    "files = sorted([p for p in CLEANED_DIR.glob('Asset_*.csv')])\n"
                    "symbols = [p.stem for p in files][:ASSET_LIMIT]\n"
                    "assets_ohlcv = load_cleaned_assets(symbols=symbols, cleaned_dir=str(CLEANED_DIR))\n\n"
                    "frames = []\n"
                    "for sym, df in assets_ohlcv.items():\n"
                    "    feat = compute_features_fast(df)\n"
                    "    feat['Asset_ID'] = sym\n"
                    "    frames.append(feat)\n"
                    "data = pd.concat(frames, axis=0).sort_index().dropna(subset=['y_up_fwd'])\n\n"
                    "# Time split\n"
                    "TRAIN_YEARS = 7\nVAL_MONTHS = 18\nTEST_MONTHS = 18\n\n"
                    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n"
                    "    pos = int(index.searchsorted(ts, side='left'))\n"
                    "    if pos >= len(index):\n"
                    "        return pd.Timestamp(index[-1])\n"
                    "    return pd.Timestamp(index[pos])\n\n"
                    "idx = pd.DatetimeIndex(data.index.unique()).sort_values()\n"
                    "end = pd.Timestamp(idx[-1])\n"
                    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n"
                    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n"
                    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n"
                    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n"
                    "val_start = align_to_trading_date(idx, pd.Timestamp(raw_val_start))\n"
                    "train_start = align_to_trading_date(idx, pd.Timestamp(raw_train_start))\n\n"
                    "df_train = data.loc[(data.index >= train_start) & (data.index < val_start)].copy()\n"
                    "df_test = data.loc[(data.index >= test_start) & (data.index <= end)].copy()\n\n"
                    "feature_cols = [c for c in df_train.columns if c not in {'Asset_ID', 'ret_1d_fwd', 'y_up_fwd'}]\n"
                    "X_train = df_train[feature_cols].replace([np.inf, -np.inf], np.nan)\n"
                    "y_train = df_train['y_up_fwd'].astype(int).to_numpy()\n"
                    "X_test = df_test[feature_cols].replace([np.inf, -np.inf], np.nan)\n\n"
                    "imp = SimpleImputer(strategy='median')\n"
                    "scaler = StandardScaler()\n"
                    "Xtr = scaler.fit_transform(imp.fit_transform(X_train))\n"
                    "Xte = scaler.transform(imp.transform(X_test))\n"
                    "Xtr = np.hstack([np.ones((Xtr.shape[0], 1)), Xtr])\n"
                    "Xte = np.hstack([np.ones((Xte.shape[0], 1)), Xte])\n\n"
                    "# Subsample for speed\n"
                    "if Xtr.shape[0] > N_TRAIN_SAMPLES:\n"
                    "    idx_s = rng.choice(Xtr.shape[0], size=N_TRAIN_SAMPLES, replace=False)\n"
                    "    Xtr = Xtr[idx_s]\n"
                    "    y_train = y_train[idx_s]\n\n"
                    "d = Xtr.shape[1]\n\n"
                    "# Adam optimizer\n"
                    "mu = np.zeros(d)\n"
                    "log_var = np.full(d, -2.0)\n"
                    "m_mu = np.zeros(d); v_mu = np.zeros(d)\n"
                    "m_lv = np.zeros(d); v_lv = np.zeros(d)\n"
                    "b1 = 0.9; b2 = 0.999; eps = 1e-8\n\n"
                    "def elbo_grad(mu: np.ndarray, log_var: np.ndarray, Xb: np.ndarray, yb: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n"
                    "    # Single-sample Monte Carlo gradient via reparameterization\n"
                    "    std = np.exp(0.5 * log_var)\n"
                    "    z = rng.normal(size=mu.shape)\n"
                    "    beta = mu + std * z\n"
                    "    logits = Xb @ beta\n"
                    "    p = expit(logits)\n"
                    "    # loglik grad wrt beta\n"
                    "    g_beta = Xb.T @ (yb - p)\n"
                    "    # prior grad (no prior on intercept)\n"
                    "    g_beta[1:] += -(beta[1:] / TAU2)\n"
                    "    # q entropy term: grad of -log q = + (beta-mu)/var for mu, etc; reparam handles it\n"
                    "    # Gradient of ELBO wrt mu is g_beta\n"
                    "    g_mu = g_beta\n"
                    "    # Gradient wrt log_var: chain through std and beta = mu + std*z\n"
                    "    g_std = g_beta * z\n"
                    "    g_log_var = 0.5 * std * g_std + 0.5  # +0.5 from entropy of diagonal Gaussian\n"
                    "    g_log_var[0] = 0.5 * std[0] * g_std[0] + 0.5\n"
                    "    return g_mu, g_log_var\n\n"
                    "# Optimize\n"
                    "n = Xtr.shape[0]\n"
                    "for t in range(1, N_STEPS + 1):\n"
                    "    idx_b = rng.integers(0, n, size=BATCH)\n"
                    "    Xb = Xtr[idx_b]\n"
                    "    yb = y_train[idx_b]\n"
                    "    g_mu, g_lv = elbo_grad(mu, log_var, Xb, yb)\n"
                    "    # Adam updates\n"
                    "    m_mu = b1 * m_mu + (1 - b1) * g_mu\n"
                    "    v_mu = b2 * v_mu + (1 - b2) * (g_mu * g_mu)\n"
                    "    m_lv = b1 * m_lv + (1 - b1) * g_lv\n"
                    "    v_lv = b2 * v_lv + (1 - b2) * (g_lv * g_lv)\n"
                    "    m_mu_hat = m_mu / (1 - b1 ** t)\n"
                    "    v_mu_hat = v_mu / (1 - b2 ** t)\n"
                    "    m_lv_hat = m_lv / (1 - b1 ** t)\n"
                    "    v_lv_hat = v_lv / (1 - b2 ** t)\n"
                    "    mu += LR * m_mu_hat / (np.sqrt(v_mu_hat) + eps)\n"
                    "    log_var += LR * m_lv_hat / (np.sqrt(v_lv_hat) + eps)\n"
                    "    if t % 300 == 0:\n"
                    "        print('step', t, 'mean std(beta):', float(np.mean(np.exp(0.5 * log_var))))\n\n"
                    "# Predict by sampling from q\n"
                    "S = 200\n"
                    "std = np.exp(0.5 * log_var)\n"
                    "B = mu + std * rng.normal(size=(S, d))\n"
                    "p_samps = expit(Xte @ B.T)\n"
                    "p_mean = p_samps.mean(axis=1)\n\n"
                    "pred_long = pd.DataFrame({'Date': df_test.index, 'Asset_ID': df_test['Asset_ID'].to_numpy(), 'signal': p_mean - 0.5})\n"
                    "pred_matrix = pred_long.pivot_table(index='Date', columns='Asset_ID', values='signal', aggfunc='mean').sort_index()\n"
                ),
                _backtest_cell(title="Bayes Logistic Regression (VI)"),
            ],
        )
    )

    # 5) Bayesian Sharpe ratio (posterior over mean+vol)
    specs.append(
        NotebookSpec(
            filename="05_bayesian_sharpe_ratio_asset_selection.ipynb",
            title="# Bayesian Sharpe Ratio (Posterior Ranking for Asset Selection)",
            intro_md=(
                "Chapter 10 models the Sharpe ratio as a probabilistic quantity. Here we use a\n"
                "conjugate Normal-Inverse-Gamma model for daily returns to obtain a posterior\n"
                "over $(\\mu, \\sigma^2)$ and then approximate the Sharpe distribution via Monte Carlo.\n\n"
                "We rank assets by $P(\\text{Sharpe} > 0)$ on each rebalance date.\n"
            ),
            cells=[
                *_common_preamble(method_name="Bayesian Sharpe Ratio"),
                _code(
                    "from backtester.data import load_cleaned_assets, align_close_prices\n\n"
                    "assets_ohlcv = load_cleaned_assets(cleaned_dir=str(CLEANED_DIR))\n"
                    "close_prices = align_close_prices(assets_ohlcv).sort_index()\n"
                    "rets = close_prices.pct_change().dropna(how='all').fillna(0.0)\n\n"
                    "# Time split\n"
                    "TRAIN_YEARS = 7\nVAL_MONTHS = 18\nTEST_MONTHS = 18\n\n"
                    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n"
                    "    pos = int(index.searchsorted(ts, side='left'))\n"
                    "    if pos >= len(index):\n"
                    "        return pd.Timestamp(index[-1])\n"
                    "    return pd.Timestamp(index[pos])\n\n"
                    "idx = pd.DatetimeIndex(rets.index).sort_values()\n"
                    "end = pd.Timestamp(idx[-1])\n"
                    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n"
                    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n"
                    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n"
                    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n"
                    "val_start = align_to_trading_date(idx, pd.Timestamp(raw_val_start))\n"
                    "train_start = align_to_trading_date(idx, pd.Timestamp(raw_train_start))\n\n"
                    "# NIG hyperparameters (weakly informative)\n"
                    "mu0 = 0.0\n"
                    "kappa0 = 1.0\n"
                    "alpha0 = 2.0\n"
                    "beta0 = 1e-4\n\n"
                    "LOOKBACK = 252\n"
                    "S = 300\n"
                    "REBALANCE_FREQ = 'W'\n\n"
                    "# Build prediction matrix on the test window: score = P(Sharpe>0) - 0.5\n"
                    "test_idx = rets.loc[test_start:end].index\n"
                    "rebal_dates = pd.Series(test_idx, index=test_idx).resample(REBALANCE_FREQ).last().dropna().tolist()\n"
                    "score_rebal = pd.DataFrame(index=pd.DatetimeIndex(rebal_dates), columns=rets.columns, dtype=float)\n\n"
                    "def posterior_params(window: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n"
                    "    x = window.to_numpy(dtype=float)\n"
                    "    valid = np.isfinite(x)\n"
                    "    n = valid.sum(axis=0).astype(float)\n"
                    "    x0 = np.where(valid, x, 0.0)\n"
                    "    xbar = np.divide(x0.sum(axis=0), np.maximum(1.0, n))\n"
                    "    centered = np.where(valid, x0 - xbar, 0.0)\n"
                    "    ss = (centered * centered).sum(axis=0)\n"
                    "    denom = np.maximum(1.0, n - 1.0)\n"
                    "    s2 = ss / denom\n"
                    "    kappa_n = kappa0 + n\n"
                    "    mu_n = (kappa0 * mu0 + n * xbar) / np.maximum(1e-12, kappa_n)\n"
                    "    alpha_n = alpha0 + 0.5 * n\n"
                    "    beta_n = beta0 + 0.5 * (n - 1.0) * s2 + 0.5 * (kappa0 * n / np.maximum(1e-12, kappa_n)) * (xbar - mu0) ** 2\n"
                    "    return mu_n, kappa_n, alpha_n, beta_n\n\n"
                    "for dt in score_rebal.index:\n"
                    "    end_pos = int(rets.index.searchsorted(dt, side='left'))\n"
                    "    start_pos = max(0, end_pos - LOOKBACK)\n"
                    "    window = rets.iloc[start_pos:end_pos]\n"
                    "    mu_n, kappa_n, alpha_n, beta_n = posterior_params(window)\n"
                    "    g = rng.gamma(shape=alpha_n[None, :], scale=1.0 / beta_n[None, :], size=(S, alpha_n.shape[0]))\n"
                    "    sigma2 = 1.0 / g\n"
                    "    mu = rng.normal(loc=mu_n[None, :], scale=np.sqrt(sigma2 / kappa_n[None, :]))\n"
                    "    sharpe = mu / np.sqrt(sigma2 + 1e-18)\n"
                    "    p_pos = (sharpe > 0.0).mean(axis=0)\n"
                    "    score_rebal.loc[dt] = p_pos - 0.5\n\n"
                    "pred_matrix = score_rebal.reindex(test_idx, method='ffill').fillna(0.0)\n"
                ),
                _backtest_cell(title="Bayes Sharpe Ratio (P[SR>0])"),
            ],
        )
    )

    # 6) Bayesian rolling regression (pairs)
    specs.append(
        NotebookSpec(
            filename="06_bayesian_rolling_regression_pairs_trading.ipynb",
            title="# Bayesian Rolling Regression (Pairs Trading Hedge Ratio)",
            intro_md=(
                "We estimate a time-varying hedge ratio between two assets using Bayesian\n"
                "linear regression in a rolling window. The conjugate Normal-Inverse-Gamma\n"
                "posterior provides both a point estimate and uncertainty for the slope.\n\n"
                "Trading rule (simple):\n"
                "- compute spread $s_t = y_t - \\hat{\\beta}_t x_t$\n"
                "- z-score the spread over the rolling window\n"
                "- long/short when |z| exceeds a threshold\n"
            ),
            cells=[
                *_common_preamble(method_name="Bayesian Rolling Regression"),
                _code(
                    "from backtester.data import load_cleaned_assets, align_close_prices\n\n"
                    "# Pick a deterministic pair (keep reproducible).\n"
                    "PAIR = ('Asset_001', 'Asset_002')\n"
                    "assets_ohlcv = load_cleaned_assets(symbols=list(PAIR), cleaned_dir=str(CLEANED_DIR))\n"
                    "close_prices = align_close_prices(assets_ohlcv).sort_index()\n\n"
                    "y = np.log(close_prices[PAIR[0]].replace(0.0, np.nan)).dropna()\n"
                    "x = np.log(close_prices[PAIR[1]].replace(0.0, np.nan)).dropna()\n"
                    "idx = y.index.intersection(x.index)\n"
                    "y = y.loc[idx].astype(float)\n"
                    "x = x.loc[idx].astype(float)\n\n"
                    "# Time split\n"
                    "TRAIN_YEARS = 7\nVAL_MONTHS = 18\nTEST_MONTHS = 18\n\n"
                    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n"
                    "    pos = int(index.searchsorted(ts, side='left'))\n"
                    "    if pos >= len(index):\n"
                    "        return pd.Timestamp(index[-1])\n"
                    "    return pd.Timestamp(index[pos])\n\n"
                    "end = pd.Timestamp(idx[-1])\n"
                    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n"
                    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n"
                    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n"
                    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n\n"
                    "# Conjugate prior for slope-only regression y = beta x + eps (no intercept for simplicity).\n"
                    "beta0 = 1.0\n"
                    "V0 = 1.0  # prior variance on beta\n"
                    "alpha0 = 2.0\n"
                    "beta_ig0 = 1.0\n\n"
                    "WINDOW = 126\n"
                    "Z_ENTRY = 1.5\n\n"
                    "beta_hat = pd.Series(index=idx, dtype=float)\n"
                    "spread = pd.Series(index=idx, dtype=float)\n"
                    "for t in range(len(idx)):\n"
                    "    dt = idx[t]\n"
                    "    start = max(0, t - WINDOW)\n"
                    "    xs = x.iloc[start:t].to_numpy()\n"
                    "    ys = y.iloc[start:t].to_numpy()\n"
                    "    if xs.size < 20:\n"
                    "        beta_hat.loc[dt] = np.nan\n"
                    "        spread.loc[dt] = np.nan\n"
                    "        continue\n"
                    "    XTX = float(np.sum(xs * xs))\n"
                    "    XTy = float(np.sum(xs * ys))\n"
                    "    Vn = 1.0 / (1.0 / V0 + XTX)\n"
                    "    beta_n = Vn * ((beta0 / V0) + XTy)\n"
                    "    # Residual sum of squares using posterior mean\n"
                    "    rss = float(np.sum((ys - beta_n * xs) ** 2))\n"
                    "    alpha_n = alpha0 + 0.5 * xs.size\n"
                    "    beta_ig_n = beta_ig0 + 0.5 * rss\n"
                    "    beta_hat.loc[dt] = float(beta_n)\n"
                    "    spread.loc[dt] = float(y.loc[dt] - beta_n * x.loc[dt])\n\n"
                    "# Z-score of spread\n"
                    "z = (spread - spread.rolling(WINDOW).mean()) / spread.rolling(WINDOW).std()\n\n"
                    "# Build weights: long y short x when z < -Z_ENTRY; reverse when z > Z_ENTRY\n"
                    "pred_matrix = pd.DataFrame(index=idx[idx >= test_start], columns=list(PAIR), dtype=float)\n"
                    "for dt in pred_matrix.index:\n"
                    "    zz = float(z.loc[dt])\n"
                    "    if not np.isfinite(zz):\n"
                    "        pred_matrix.loc[dt] = 0.0\n"
                    "    elif zz > Z_ENTRY:\n"
                    "        # short y, long x\n"
                    "        pred_matrix.loc[dt, PAIR[0]] = -1.0\n"
                    "        pred_matrix.loc[dt, PAIR[1]] = +1.0\n"
                    "    elif zz < -Z_ENTRY:\n"
                    "        # long y, short x\n"
                    "        pred_matrix.loc[dt, PAIR[0]] = +1.0\n"
                    "        pred_matrix.loc[dt, PAIR[1]] = -1.0\n"
                    "    else:\n"
                    "        pred_matrix.loc[dt] = 0.0\n\n"
                    "# For the shared backtest cell, we want signals (ranking), not fixed +/-1 weights.\n"
                    "# Convert to ranking-like signals: long leg gets +1, short leg gets -1.\n"
                    "# The weight builder will select positives only; to preserve long-short, we run the backtest directly below.\n"
                ),
                _code(
                    "# Backtest the long-short directly using the engine (skip top-K selection).\n"
                    "from IPython.display import display\n"
                    "from bokeh.io import output_notebook, show\n\n"
                    "from backtester.engine import BacktestConfig, run_backtest\n"
                    "from backtester.report import compute_backtest_report\n"
                    "from backtester.bokeh_plots import build_interactive_portfolio_layout\n\n"
                    "output_notebook()\n\n"
                    "cfg = BacktestConfig(initial_equity=1_000_000.0, transaction_cost_bps=5.0, mode='vectorized', allow_leverage=False)\n"
                    "# Normalize gross exposure to 1.0\n"
                    "w = pred_matrix.copy().fillna(0.0)\n"
                    "gross = w.abs().sum(axis=1).replace(0.0, np.nan)\n"
                    "w = w.div(gross, axis=0).fillna(0.0)\n\n"
                    "# Align close prices\n"
                    "close_bt = close_prices.loc[w.index, list(PAIR)]\n"
                    "res = run_backtest(close_bt, w, config=cfg)\n"
                    "rpt = compute_backtest_report(result=res, close_prices=close_bt)\n"
                    "display(rpt.to_frame('Bayes Rolling Regression (Pairs) - Long/Short'))\n\n"
                    "market_df = pd.DataFrame({\n"
                    "    'Open': pd.concat([df['Open'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n"
                    "    'High': pd.concat([df['High'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n"
                    "    'Low': pd.concat([df['Low'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n"
                    "    'Close': pd.concat([df['Close'] for df in assets_ohlcv.values()], axis=1).mean(axis=1),\n"
                    "    'Volume': pd.concat([df['Volume'] for df in assets_ohlcv.values()], axis=1).sum(axis=1),\n"
                    "}).sort_index().loc[w.index]\n\n"
                    "layout = build_interactive_portfolio_layout(\n"
                    "    market_ohlcv=market_df,\n"
                    "    equity=res.equity,\n"
                    "    returns=res.returns,\n"
                    "    weights=res.weights,\n"
                    "    turnover=res.turnover,\n"
                    "    costs=res.costs,\n"
                    "    close_prices=close_bt,\n"
                    "    title='Bayes Rolling Regression (Pairs) - Long/Short',\n"
                    ")\n"
                    "show(layout)\n"
                ),
            ],
        )
    )

    # 7) Stochastic volatility (particle filter) -> volatility-managed allocation
    specs.append(
        NotebookSpec(
            filename="07_stochastic_volatility_particle_filter.ipynb",
            title="# Stochastic Volatility (Particle Filter) for Volatility Forecasting",
            intro_md=(
                "Chapter 10 introduces stochastic volatility models with latent volatility.\n\n"
                "We use a simple state-space model:\n\n"
                "- Latent log-vol: $h_t = \\mu + \\phi (h_{t-1} - \\mu) + \\eta_t$, $\\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$\n"
                "- Returns: $r_t \\sim \\mathcal{N}(0, \\exp(h_t))$\n\n"
                "We run a bootstrap particle filter (SMC) with fixed parameters to estimate\n"
                "filtered volatility. The forecasted volatility is used to create a\n"
                "volatility-managed signal.\n"
            ),
            cells=[
                *_common_preamble(
                    method_name="Stochastic Volatility (Particle Filter)"
                ),
                _code(
                    "from backtester.data import load_cleaned_assets, align_close_prices\n\n"
                    "ASSET_LIMIT = 30\n"
                    "N_PARTICLES = 400\n\n"
                    "assets_ohlcv = load_cleaned_assets(cleaned_dir=str(CLEANED_DIR))\n"
                    "close_prices_full = align_close_prices(assets_ohlcv).sort_index()\n"
                    "assets = list(close_prices_full.columns)[:ASSET_LIMIT]\n"
                    "close_prices = close_prices_full.loc[:, assets]\n\n"
                    "rets = np.log(close_prices / close_prices.shift(1)).replace([np.inf, -np.inf], np.nan).dropna(how='all')\n"
                    "rets = rets.fillna(0.0)\n\n"
                    "# Time split\n"
                    "TRAIN_YEARS = 7\nVAL_MONTHS = 18\nTEST_MONTHS = 18\n\n"
                    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n"
                    "    pos = int(index.searchsorted(ts, side='left'))\n"
                    "    if pos >= len(index):\n"
                    "        return pd.Timestamp(index[-1])\n"
                    "    return pd.Timestamp(index[pos])\n\n"
                    "idx = pd.DatetimeIndex(rets.index).sort_values()\n"
                    "end = pd.Timestamp(idx[-1])\n"
                    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n"
                    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n"
                    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n"
                    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n\n"
                    "# Fixed SV parameters (simple, robust defaults)\n"
                    "phi = 0.98\n"
                    "sigma_eta = 0.15\n\n"
                    "def particle_filter_sv(r: np.ndarray) -> np.ndarray:\n"
                    "    # Returns filtered vol (std) per time index\n"
                    "    r = np.asarray(r, dtype=float)\n"
                    "    # Initialize mu from empirical variance\n"
                    "    v = float(np.var(r))\n"
                    "    mu = float(np.log(v + 1e-12))\n"
                    "    h = rng.normal(loc=mu, scale=0.5, size=N_PARTICLES)\n"
                    "    w = np.full(N_PARTICLES, 1.0 / N_PARTICLES)\n"
                    "    vol = np.zeros(r.shape[0], dtype=float)\n"
                    "    for t in range(r.shape[0]):\n"
                    "        # propagate\n"
                    "        h = mu + phi * (h - mu) + rng.normal(0.0, sigma_eta, size=N_PARTICLES)\n"
                    "        # weight by likelihood N(0, exp(h))\n"
                    "        var = np.exp(h)\n"
                    "        ll = -0.5 * (np.log(2 * np.pi * var) + (r[t] ** 2) / var)\n"
                    "        ll = ll - np.max(ll)\n"
                    "        w = np.exp(ll)\n"
                    "        w = w / np.sum(w)\n"
                    "        # resample (systematic)\n"
                    "        cdf = np.cumsum(w)\n"
                    "        u0 = rng.random() / N_PARTICLES\n"
                    "        u = u0 + np.arange(N_PARTICLES) / N_PARTICLES\n"
                    "        idxs = np.searchsorted(cdf, u)\n"
                    "        h = h[idxs]\n"
                    "        w = np.full(N_PARTICLES, 1.0 / N_PARTICLES)\n"
                    "        vol[t] = float(np.sqrt(np.exp(np.mean(h))))\n"
                    "    return vol\n\n"
                    "# Compute filtered vol per asset (slow but manageable for ASSET_LIMIT)\n"
                    "vol_filt = pd.DataFrame(index=rets.index, columns=assets, dtype=float)\n"
                    "for a in assets:\n"
                    "    vol_filt[a] = particle_filter_sv(rets[a].to_numpy())\n\n"
                    "# Predictive signal: 5-day momentum scaled by next-day volatility forecast (use filtered vol as forecast)\n"
                    "mom_5d = rets.rolling(5).sum()\n"
                    "score = (mom_5d / (vol_filt + 1e-12)).loc[test_start:end]\n"
                    "pred_matrix = score.fillna(0.0)\n"
                ),
                _backtest_cell(title="Stochastic Volatility (Particle Filter)"),
            ],
        )
    )

    # 8) MCMC: NUTS (No-U-Turn Sampler) on a small logistic regression
    specs.append(
        NotebookSpec(
            filename="08_bayesian_logistic_regression_mcmc_nuts.ipynb",
            title="# Bayesian Logistic Regression (MCMC: NUTS Sampler)",
            intro_md=(
                "Chapter 10 highlights the No-U-Turn Sampler (NUTS), an adaptive variant of\n"
                "Hamiltonian Monte Carlo (HMC).\n\n"
                "This notebook implements a compact NUTS sampler (identity mass matrix) for\n"
                "Bayesian logistic regression on a **small** training set, and then produces\n"
                "out-of-sample predictions and runs the shared backtest.\n\n"
                "Note: NUTS is computationally expensive; we intentionally limit the universe\n"
                "and training samples to keep runtime reasonable.\n"
            ),
            cells=[
                *_common_preamble(
                    method_name="Bayesian Logistic Regression (MCMC: NUTS)"
                ),
                _code(
                    "from scipy.special import expit\n"
                    "from sklearn.impute import SimpleImputer\n"
                    "from sklearn.preprocessing import StandardScaler\n\n"
                    "ASSET_LIMIT = 15\n"
                    "N_TRAIN_SAMPLES = 6_000\n"
                    "WARMUP = 300\n"
                    "SAMPLES = 300\n"
                    "MAX_DEPTH = 8\n"
                    "TARGET_ACCEPT = 0.65\n"
                    "TAU2 = 10.0**2\n\n"
                    "def compute_features_fast(ohlcv: pd.DataFrame) -> pd.DataFrame:\n"
                    "    df = ohlcv.copy().sort_index()\n"
                    "    c = df['Close'].astype(float)\n"
                    "    h = df['High'].astype(float)\n"
                    "    l = df['Low'].astype(float)\n"
                    "    v = df['Volume'].astype(float)\n"
                    "    ret_1d = c.pct_change()\n"
                    "    hl_range_pct = (h - l) / c.replace(0.0, np.nan)\n"
                    "    vol_z_20 = (v - v.rolling(20).mean()) / v.rolling(20).std()\n"
                    "    out = pd.DataFrame({'ret_1d': ret_1d, 'hl_range_pct': hl_range_pct, 'vol_z_20': vol_z_20}).sort_index()\n"
                    "    out['ret_1d_fwd'] = out['ret_1d'].shift(-1)\n"
                    "    out['y_up_fwd'] = (out['ret_1d_fwd'] > 0).astype(int)\n"
                    "    return out\n\n"
                    "from backtester.data import load_cleaned_assets\n"
                    "files = sorted([p for p in CLEANED_DIR.glob('Asset_*.csv')])\n"
                    "symbols = [p.stem for p in files][:ASSET_LIMIT]\n"
                    "assets_ohlcv = load_cleaned_assets(symbols=symbols, cleaned_dir=str(CLEANED_DIR))\n\n"
                    "frames = []\n"
                    "for sym, df in assets_ohlcv.items():\n"
                    "    feat = compute_features_fast(df)\n"
                    "    feat['Asset_ID'] = sym\n"
                    "    frames.append(feat)\n"
                    "data = pd.concat(frames, axis=0).sort_index().dropna(subset=['y_up_fwd'])\n\n"
                    "# Time split\n"
                    "TRAIN_YEARS = 7\nVAL_MONTHS = 18\nTEST_MONTHS = 18\n\n"
                    "def align_to_trading_date(index: pd.DatetimeIndex, ts: pd.Timestamp) -> pd.Timestamp:\n"
                    "    pos = int(index.searchsorted(ts, side='left'))\n"
                    "    if pos >= len(index):\n"
                    "        return pd.Timestamp(index[-1])\n"
                    "    return pd.Timestamp(index[pos])\n\n"
                    "idx = pd.DatetimeIndex(data.index.unique()).sort_values()\n"
                    "end = pd.Timestamp(idx[-1])\n"
                    "raw_test_start = end - pd.DateOffset(months=TEST_MONTHS)\n"
                    "raw_val_start = raw_test_start - pd.DateOffset(months=VAL_MONTHS)\n"
                    "raw_train_start = raw_val_start - pd.DateOffset(years=TRAIN_YEARS)\n"
                    "test_start = align_to_trading_date(idx, pd.Timestamp(raw_test_start))\n"
                    "val_start = align_to_trading_date(idx, pd.Timestamp(raw_val_start))\n"
                    "train_start = align_to_trading_date(idx, pd.Timestamp(raw_train_start))\n\n"
                    "df_train = data.loc[(data.index >= train_start) & (data.index < val_start)].copy()\n"
                    "df_test = data.loc[(data.index >= test_start) & (data.index <= end)].copy()\n\n"
                    "feature_cols = [c for c in df_train.columns if c not in {'Asset_ID', 'ret_1d_fwd', 'y_up_fwd'}]\n"
                    "X_train = df_train[feature_cols].replace([np.inf, -np.inf], np.nan)\n"
                    "y_train = df_train['y_up_fwd'].astype(int).to_numpy()\n"
                    "X_test = df_test[feature_cols].replace([np.inf, -np.inf], np.nan)\n\n"
                    "imp = SimpleImputer(strategy='median')\n"
                    "scaler = StandardScaler()\n"
                    "Xtr = scaler.fit_transform(imp.fit_transform(X_train))\n"
                    "Xte = scaler.transform(imp.transform(X_test))\n"
                    "Xtr = np.hstack([np.ones((Xtr.shape[0], 1)), Xtr])\n"
                    "Xte = np.hstack([np.ones((Xte.shape[0], 1)), Xte])\n\n"
                    "# Subsample for NUTS\n"
                    "if Xtr.shape[0] > N_TRAIN_SAMPLES:\n"
                    "    idx_s = rng.choice(Xtr.shape[0], size=N_TRAIN_SAMPLES, replace=False)\n"
                    "    Xtr = Xtr[idx_s]\n"
                    "    y_train = y_train[idx_s]\n\n"
                    "d = Xtr.shape[1]\n\n"
                    "def logp_and_grad(theta: np.ndarray) -> tuple[float, np.ndarray]:\n"
                    "    # log posterior up to constant\n"
                    "    z = Xtr @ theta\n"
                    "    p = expit(z)\n"
                    "    eps = 1e-12\n"
                    "    ll = float(np.sum(y_train * np.log(p + eps) + (1 - y_train) * np.log(1 - p + eps)))\n"
                    "    lp = float(-0.5 * np.sum(theta[1:] ** 2) / TAU2)\n"
                    "    # gradient\n"
                    "    g = Xtr.T @ (y_train - p)\n"
                    "    g[1:] += -(theta[1:] / TAU2)\n"
                    "    return ll + lp, g\n\n"
                    "def leapfrog(theta: np.ndarray, r: np.ndarray, grad: np.ndarray, eps: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n"
                    "    r_half = r + 0.5 * eps * grad\n"
                    "    theta_new = theta + eps * r_half\n"
                    "    lp_new, grad_new = logp_and_grad(theta_new)\n"
                    "    r_new = r_half + 0.5 * eps * grad_new\n"
                    "    return theta_new, r_new, grad_new, lp_new\n\n"
                    "def stop_criterion(theta_minus: np.ndarray, theta_plus: np.ndarray, r_minus: np.ndarray, r_plus: np.ndarray) -> bool:\n"
                    "    dtheta = theta_plus - theta_minus\n"
                    "    return (np.dot(dtheta, r_minus) >= 0.0) and (np.dot(dtheta, r_plus) >= 0.0)\n\n"
                    "def build_tree(theta: np.ndarray, r: np.ndarray, grad: np.ndarray, log_u: float, v: int, j: int, eps: float, theta0: np.ndarray, r0: np.ndarray, lp0: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, bool, float, int]:\n"
                    "    if j == 0:\n"
                    "        theta1, r1, grad1, lp1 = leapfrog(theta, r, grad, v * eps)\n"
                    "        joint = lp1 - 0.5 * np.dot(r1, r1)\n"
                    "        n1 = 1 if log_u <= joint else 0\n"
                    "        s1 = (log_u < (joint + 1000.0))\n"
                    "        alpha = min(1.0, math.exp(joint - (lp0 - 0.5 * np.dot(r0, r0))))\n"
                    "        return theta1, r1, theta1, r1, theta1, n1, s1, alpha, 1\n"
                    "    theta_minus, r_minus, theta_plus, r_plus, theta1, n1, s1, alpha1, n_alpha1 = build_tree(theta, r, grad, log_u, v, j - 1, eps, theta0, r0, lp0)\n"
                    "    if s1:\n"
                    "        if v == -1:\n"
                    "            theta_minus, r_minus, _, _, theta2, n2, s2, alpha2, n_alpha2 = build_tree(theta_minus, r_minus, logp_and_grad(theta_minus)[1], log_u, v, j - 1, eps, theta0, r0, lp0)\n"
                    "        else:\n"
                    "            _, _, theta_plus, r_plus, theta2, n2, s2, alpha2, n_alpha2 = build_tree(theta_plus, r_plus, logp_and_grad(theta_plus)[1], log_u, v, j - 1, eps, theta0, r0, lp0)\n"
                    "        if (n1 + n2) > 0 and rng.random() < (n2 / (n1 + n2)):\n"
                    "            theta1 = theta2\n"
                    "        s1 = s2 and stop_criterion(theta_minus, theta_plus, r_minus, r_plus)\n"
                    "        alpha1 += alpha2\n"
                    "        n_alpha1 += n_alpha2\n"
                    "        n1 += n2\n"
                    "    return theta_minus, r_minus, theta_plus, r_plus, theta1, n1, s1, alpha1, n_alpha1\n\n"
                    "# Dual averaging for step size during warmup\n"
                    "eps = 0.1\n"
                    "mu_da = math.log(10.0 * eps)\n"
                    "hbar = 0.0\n"
                    "eps_bar = 1.0\n"
                    "gamma = 0.05\n"
                    "t0 = 10.0\n"
                    "kappa = 0.75\n\n"
                    "theta = np.zeros(d)\n"
                    "lp, grad = logp_and_grad(theta)\n"
                    "draws = []\n"
                    "for m in range(1, WARMUP + SAMPLES + 1):\n"
                    "    r0 = rng.normal(size=d)\n"
                    "    joint0 = lp - 0.5 * np.dot(r0, r0)\n"
                    "    log_u = joint0 + math.log(rng.random())\n"
                    "    theta_minus = theta.copy(); theta_plus = theta.copy()\n"
                    "    r_minus = r0.copy(); r_plus = r0.copy()\n"
                    "    theta_prop = theta.copy()\n"
                    "    n = 1\n"
                    "    s = True\n"
                    "    alpha = 0.0\n"
                    "    n_alpha = 0\n"
                    "    for j in range(MAX_DEPTH):\n"
                    "        v = -1 if rng.random() < 0.5 else 1\n"
                    "        if v == -1:\n"
                    "            theta_minus, r_minus, _, _, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime = build_tree(theta_minus, r_minus, logp_and_grad(theta_minus)[1], log_u, v, j, eps, theta, r0, lp)\n"
                    "        else:\n"
                    "            _, _, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime = build_tree(theta_plus, r_plus, logp_and_grad(theta_plus)[1], log_u, v, j, eps, theta, r0, lp)\n"
                    "        if not s_prime:\n"
                    "            break\n"
                    "        if rng.random() < (n_prime / n):\n"
                    "            theta_prop = theta_prime\n"
                    "        n += n_prime\n"
                    "        s = s_prime and stop_criterion(theta_minus, theta_plus, r_minus, r_plus)\n"
                    "        alpha += alpha_prime\n"
                    "        n_alpha += n_alpha_prime\n"
                    "        if not s:\n"
                    "            break\n"
                    "    theta = theta_prop\n"
                    "    lp, grad = logp_and_grad(theta)\n"
                    "    accept_rate = alpha / max(1, n_alpha)\n"
                    "    if m <= WARMUP:\n"
                    "        hbar = (1 - 1 / (m + t0)) * hbar + (1 / (m + t0)) * (TARGET_ACCEPT - accept_rate)\n"
                    "        log_eps = mu_da - (math.sqrt(m) / gamma) * hbar\n"
                    "        eps = math.exp(log_eps)\n"
                    "        eta = m ** (-kappa)\n"
                    "        eps_bar = math.exp((1 - eta) * math.log(eps_bar) + eta * log_eps)\n"
                    "        if m % 100 == 0:\n"
                    "            print('warmup', m, 'eps', eps, 'accept', accept_rate)\n"
                    "    else:\n"
                    "        draws.append(theta.copy())\n"
                    "\n"
                    "B = np.stack(draws, axis=0)\n"
                    "print('posterior draws:', B.shape)\n\n"
                    "# Posterior predictive\n"
                    "p_samps = expit(Xte @ B.T)\n"
                    "p_mean = p_samps.mean(axis=1)\n"
                    "pred_long = pd.DataFrame({'Date': df_test.index, 'Asset_ID': df_test['Asset_ID'].to_numpy(), 'signal': p_mean - 0.5})\n"
                    "pred_matrix = pred_long.pivot_table(index='Date', columns='Asset_ID', values='signal', aggfunc='mean').sort_index()\n"
                ),
                _backtest_cell(title="Bayes Logistic Regression (NUTS)"),
            ],
        )
    )

    # Write all notebooks
    written: list[Path] = []
    for spec in specs:
        nb = _nb(spec=spec)
        out_path = out_dir / spec.filename
        _write(nb, out_path)
        written.append(out_path)
    return written


def main() -> None:
    repo_root = Path(__file__).resolve().parents[1]
    out_dir = (
        repo_root
        / "ml_finance_thoery"
        / "machine-learning-for-trading"
        / "10_bayesian_machine_learning"
        / "predictive_models"
    )
    paths = create_notebooks(out_dir=out_dir)
    print(f"Wrote {len(paths)} notebooks:")
    for p in paths:
        print("-", p)


if __name__ == "__main__":
    main()
